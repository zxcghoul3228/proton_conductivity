{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9653c331-e290-4dde-8083-a58851cf4801",
   "metadata": {},
   "outputs": [],
   "source": [
    "from moftransformer.utils import prepare_data\n",
    "\n",
    "# Get example path\n",
    "root_cifs = \"./pcond_example/root_cifs\"\n",
    "root_dataset = \"./pcond_example/root_dataset\"\n",
    "downstream = \"pcond\"\n",
    "\n",
    "train_fraction = 0.0\n",
    "test_fraction = 1.0  # only for inference\n",
    "\n",
    "# Run prepare data\n",
    "prepare_data(\n",
    "    root_cifs,\n",
    "    root_dataset,\n",
    "    downstream=downstream,\n",
    "    train_fraction=train_fraction,\n",
    "    test_fraction=test_fraction,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8beb3bf2-b7aa-4010-be5e-ae8ef1f42006",
   "metadata": {},
   "source": [
    "Note that you should normalize global features. Below we provide mean and std values for global features used in this work. <br>\n",
    "RH: mean = 0.8625437873460142; std = 0.16382520644832796 <br>\n",
    "1000/T: mean = 3.096828824472335; std = 0.23282128469042473 <br>\n",
    "Ka: mean = 0.05705342237061772; std = 4.059215838170252 <br>\n",
    "extra_proton: mean = 0.18447412353923207; std = 0.387870366596446 <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5d2ae6d-7ab9-4592-9859-702d52748e62",
   "metadata": {},
   "source": [
    "In this example we use the following values for global features: <br>\n",
    "RH = 0.98 <br>\n",
    "T = 353.0 <br>\n",
    "Ka = -1.7 (water) <br>\n",
    "extra_proton = 0.0 (not protonated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ded558ca-b1a7-4a2f-997f-9e9f1021f732",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import copy\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "import shutil\n",
    "\n",
    "import pytorch_lightning as pl\n",
    "\n",
    "from moftransformer.config import ex\n",
    "from moftransformer.config import config as _config\n",
    "from moftransformer.utils.validation import (\n",
    "    get_valid_config,\n",
    "    get_num_devices,\n",
    "    ConfigurationError,\n",
    ")\n",
    "\n",
    "warnings.filterwarnings(\n",
    "    \"ignore\", \".*Trying to infer the `batch_size` from an ambiguous collection.*\"\n",
    ")\n",
    "\n",
    "import os\n",
    "import random\n",
    "import json\n",
    "import pickle\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "from torch.nn.functional import interpolate\n",
    "\n",
    "\n",
    "# for regression task\n",
    "N_ADD_FEATURES = 4\n",
    "\n",
    "\n",
    "# Modified dataset\n",
    "class Dataset1(torch.utils.data.Dataset):\n",
    "    def __init__(\n",
    "        self,\n",
    "        data_dir: str,\n",
    "        split: str,\n",
    "        nbr_fea_len: int,\n",
    "        draw_false_grid=True,\n",
    "        downstream=\"\",\n",
    "        tasks=[],\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Dataset for pretrained MOF.\n",
    "        Args:\n",
    "            data_dir (str): where dataset cif files and energy grid file; exist via model.utils.prepare_data.py\n",
    "            split(str) : train, test, split\n",
    "            draw_false_grid (int, optional):  how many generating false_grid_data\n",
    "            nbr_fea_len (int) : nbr_fea_len for gaussian expansion\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.data_dir = data_dir\n",
    "        self.draw_false_grid = draw_false_grid\n",
    "        self.split = split\n",
    "\n",
    "        assert split in {\"train\", \"test\", \"val\"}\n",
    "        if downstream:\n",
    "            path_file = os.path.join(data_dir, f\"{split}_{downstream}.json\")\n",
    "            path_file_features = os.path.join(\n",
    "                data_dir, f\"{split}_{downstream}_features.json\"\n",
    "            )  # path to file with additional (global) features\n",
    "        else:\n",
    "            path_file = os.path.join(data_dir, f\"{split}.json\")\n",
    "            path_file_features = os.path.join(\n",
    "                data_dir, f\"{split}_features.json\"\n",
    "            )  # path to file with additional features\n",
    "        print(f\"read {path_file}...\")\n",
    "        print(f\"read {path_file_features}...\")\n",
    "\n",
    "        if not os.path.isfile(path_file):\n",
    "            raise FileNotFoundError(\n",
    "                f\"{path_file} doesn't exist. Check 'root_dataset' in config\"\n",
    "            )\n",
    "        # if not os.path.isfile(path_file_features):\n",
    "        # self.with_add_features = False #flag for presence of add features\n",
    "        # print('file with additional features does not exist')\n",
    "        # else:\n",
    "        # self.with_add_features = True #flag for presence of add features\n",
    "\n",
    "        dict_target = json.load(open(path_file, \"r\"))\n",
    "        self.cif_ids, self.targets = zip(*dict_target.items())\n",
    "\n",
    "        dict_add_features = json.load(\n",
    "            open(path_file_features, \"r\")\n",
    "        )  # dict with add features\n",
    "        self.add_features = [dict_add_features[cif_id] for cif_id in self.cif_ids]\n",
    "\n",
    "        self.nbr_fea_len = nbr_fea_len\n",
    "\n",
    "        self.tasks = {}\n",
    "\n",
    "        for task in tasks:\n",
    "            if task in [\"mtp\", \"vfp\", \"moc\", \"bbc\"]:\n",
    "                path_file = os.path.join(data_dir, f\"{split}_{task}.json\")\n",
    "                print(f\"read {path_file}...\")\n",
    "                assert os.path.isfile(\n",
    "                    path_file\n",
    "                ), f\"{path_file} doesn't exist in {data_dir}\"\n",
    "\n",
    "                dict_task = json.load(open(path_file, \"r\"))\n",
    "                cif_ids, t = zip(*dict_task.items())\n",
    "                self.tasks[task] = list(t)\n",
    "                assert self.cif_ids == cif_ids, print(\n",
    "                    \"order of keys is different in the json file\"\n",
    "                )\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.cif_ids)\n",
    "\n",
    "    @staticmethod\n",
    "    def make_grid_data(grid_data, emin=-5000.0, emax=5000, bins=101):\n",
    "        \"\"\"\n",
    "        make grid_data within range (emin, emax) and\n",
    "        make bins with logit function\n",
    "        and digitize (0, bins)\n",
    "        ****\n",
    "            caution : 'zero' should be padding !!\n",
    "            when you change bins, heads.MPP_heads should be changed\n",
    "        ****\n",
    "        \"\"\"\n",
    "        grid_data[grid_data <= emin] = emin\n",
    "        grid_data[grid_data > emax] = emax\n",
    "\n",
    "        x = np.linspace(emin, emax, bins)\n",
    "        new_grid_data = np.digitize(grid_data, x) + 1\n",
    "\n",
    "        return new_grid_data\n",
    "\n",
    "    @staticmethod\n",
    "    def calculate_volume(a, b, c, angle_a, angle_b, angle_c):\n",
    "        a_ = np.cos(angle_a * np.pi / 180)\n",
    "        b_ = np.cos(angle_b * np.pi / 180)\n",
    "        c_ = np.cos(angle_c * np.pi / 180)\n",
    "\n",
    "        v = a * b * c * np.sqrt(1 - a_**2 - b_**2 - c_**2 + 2 * a_ * b_ * c_)\n",
    "\n",
    "        return v.item() / (60 * 60 * 60)  # normalized volume\n",
    "\n",
    "    def get_raw_grid_data(self, cif_id):\n",
    "        file_grid = os.path.join(self.data_dir, self.split, f\"{cif_id}.grid\")\n",
    "        file_griddata = os.path.join(self.data_dir, self.split, f\"{cif_id}.griddata16\")\n",
    "\n",
    "        # get grid\n",
    "        with open(file_grid, \"r\") as f:\n",
    "            lines = f.readlines()\n",
    "            a, b, c = [float(i) for i in lines[0].split()[1:]]\n",
    "            angle_a, angle_b, angle_c = [float(i) for i in lines[1].split()[1:]]\n",
    "            cell = [int(i) for i in lines[2].split()[1:]]\n",
    "\n",
    "        volume = self.calculate_volume(a, b, c, angle_a, angle_b, angle_c)\n",
    "\n",
    "        # get grid data\n",
    "        grid_data = pickle.load(open(file_griddata, \"rb\"))\n",
    "        grid_data = self.make_grid_data(grid_data)\n",
    "        grid_data = torch.FloatTensor(grid_data)\n",
    "\n",
    "        return cell, volume, grid_data\n",
    "\n",
    "    def get_grid_data(self, cif_id, draw_false_grid=False):\n",
    "        cell, volume, grid_data = self.get_raw_grid_data(cif_id)\n",
    "        ret = {\n",
    "            \"cell\": cell,\n",
    "            \"volume\": volume,\n",
    "            \"grid_data\": grid_data,\n",
    "        }\n",
    "\n",
    "        if draw_false_grid:\n",
    "            random_index = random.randint(0, len(self.cif_ids) - 1)\n",
    "            cif_id = self.cif_ids[random_index]\n",
    "            cell, volume, grid_data = self.get_raw_grid_data(cif_id)\n",
    "            ret.update(\n",
    "                {\n",
    "                    \"false_cell\": cell,\n",
    "                    \"fale_volume\": volume,\n",
    "                    \"false_grid_data\": grid_data,\n",
    "                }\n",
    "            )\n",
    "        return ret\n",
    "\n",
    "    @staticmethod\n",
    "    def get_gaussian_distance(distances, num_step, dmax, dmin=0, var=0.2):\n",
    "        \"\"\"\n",
    "        Expands the distance by Gaussian basis\n",
    "        (https://github.com/txie-93/cgcnn.git)\n",
    "        \"\"\"\n",
    "\n",
    "        assert dmin < dmax\n",
    "        _filter = np.linspace(\n",
    "            dmin, dmax, num_step\n",
    "        )  # = np.arange(dmin, dmax + step, step) with step = 0.2\n",
    "\n",
    "        return np.exp(-((distances[..., np.newaxis] - _filter) ** 2) / var**2).float()\n",
    "\n",
    "    def get_graph(self, cif_id):\n",
    "        file_graph = os.path.join(self.data_dir, self.split, f\"{cif_id}.graphdata\")\n",
    "\n",
    "        graphdata = pickle.load(open(file_graph, \"rb\"))\n",
    "        # graphdata = [\"cif_id\", \"atom_num\", \"nbr_idx\", \"nbr_dist\", \"uni_idx\", \"uni_count\"]\n",
    "        atom_num = torch.LongTensor(graphdata[1].copy())\n",
    "        nbr_idx = torch.LongTensor(graphdata[2].copy()).view(len(atom_num), -1)\n",
    "        nbr_dist = torch.FloatTensor(graphdata[3].copy()).view(len(atom_num), -1)\n",
    "\n",
    "        nbr_fea = torch.FloatTensor(\n",
    "            self.get_gaussian_distance(nbr_dist, num_step=self.nbr_fea_len, dmax=8)\n",
    "        )\n",
    "\n",
    "        uni_idx = graphdata[4]\n",
    "        uni_count = graphdata[5]\n",
    "\n",
    "        return {\n",
    "            \"atom_num\": atom_num,\n",
    "            \"nbr_idx\": nbr_idx,\n",
    "            \"nbr_fea\": nbr_fea,\n",
    "            \"uni_idx\": uni_idx,\n",
    "            \"uni_count\": uni_count,\n",
    "        }\n",
    "\n",
    "    def get_tasks(self, index):\n",
    "        ret = dict()\n",
    "        for task, value in self.tasks.items():\n",
    "            ret.update({task: value[index]})\n",
    "\n",
    "        return ret\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        ret = dict()\n",
    "        cif_id = self.cif_ids[index]\n",
    "        target = self.targets[index]\n",
    "        add_features = self.add_features[index]  # add features list[float]\n",
    "\n",
    "        ret.update(\n",
    "            {\n",
    "                \"cif_id\": cif_id,\n",
    "                \"target\": target,\n",
    "                \"add_features\": add_features,  # add features list[float]\n",
    "            }\n",
    "        )\n",
    "        ret.update(self.get_grid_data(cif_id, draw_false_grid=self.draw_false_grid))\n",
    "        ret.update(self.get_graph(cif_id))\n",
    "\n",
    "        ret.update(self.get_tasks(index))\n",
    "\n",
    "        return ret\n",
    "\n",
    "    @staticmethod\n",
    "    def collate(batch, img_size):\n",
    "        \"\"\"\n",
    "        collate batch\n",
    "        Args:\n",
    "            batch (dict): [cif_id, atom_num, nbr_idx, nbr_fea, uni_idx, uni_count,\n",
    "                            grid_data, cell, (false_grid_data, false_cell), target]\n",
    "            img_size (int): maximum length of img size\n",
    "\n",
    "        Returns:\n",
    "            dict_batch (dict): [cif_id, atom_num, nbr_idx, nbr_fea, crystal_atom_idx,\n",
    "                                uni_idx, uni_count, grid, false_grid_data, target]\n",
    "        \"\"\"\n",
    "        batch_size = len(batch)\n",
    "        keys = set([key for b in batch for key in b.keys()])\n",
    "\n",
    "        dict_batch = {k: [dic[k] if k in dic else None for dic in batch] for k in keys}\n",
    "\n",
    "        # graph\n",
    "        batch_atom_num = dict_batch[\"atom_num\"]\n",
    "        batch_nbr_idx = dict_batch[\"nbr_idx\"]\n",
    "        batch_nbr_fea = dict_batch[\"nbr_fea\"]\n",
    "\n",
    "        crystal_atom_idx = []\n",
    "        base_idx = 0\n",
    "        for i, nbr_idx in enumerate(batch_nbr_idx):\n",
    "            n_i = nbr_idx.shape[0]\n",
    "            crystal_atom_idx.append(torch.arange(n_i) + base_idx)\n",
    "            nbr_idx += base_idx\n",
    "            base_idx += n_i\n",
    "\n",
    "        dict_batch[\"atom_num\"] = torch.cat(batch_atom_num, dim=0)\n",
    "        dict_batch[\"nbr_idx\"] = torch.cat(batch_nbr_idx, dim=0)\n",
    "        dict_batch[\"nbr_fea\"] = torch.cat(batch_nbr_fea, dim=0)\n",
    "        dict_batch[\"crystal_atom_idx\"] = crystal_atom_idx\n",
    "\n",
    "        # grid\n",
    "        batch_grid_data = dict_batch[\"grid_data\"]\n",
    "        batch_cell = dict_batch[\"cell\"]\n",
    "        new_grids = []\n",
    "\n",
    "        for bi in range(batch_size):\n",
    "            orig = batch_grid_data[bi].view(batch_cell[bi][::-1]).transpose(0, 2)\n",
    "            if batch_cell[bi] == [30, 30, 30]:  # version >= 1.1.2\n",
    "                orig = orig[None, None, :, :, :]\n",
    "            else:\n",
    "                orig = interpolate(\n",
    "                    orig[None, None, :, :, :],\n",
    "                    size=[img_size, img_size, img_size],\n",
    "                    mode=\"trilinear\",\n",
    "                    align_corners=True,\n",
    "                )\n",
    "            new_grids.append(orig)\n",
    "        new_grids = torch.concat(new_grids, axis=0)\n",
    "        dict_batch[\"grid\"] = new_grids\n",
    "\n",
    "        if \"false_grid_data\" in dict_batch.keys():\n",
    "            batch_false_grid_data = dict_batch[\"false_grid_data\"]\n",
    "            batch_false_cell = dict_batch[\"false_cell\"]\n",
    "            new_false_grids = []\n",
    "            for bi in range(batch_size):\n",
    "                orig = batch_false_grid_data[bi].view(batch_false_cell[bi])\n",
    "                if batch_cell[bi] == [30, 30, 30]:  # version >= 1.1.2\n",
    "                    orig = orig[None, None, :, :, :]\n",
    "                else:\n",
    "                    orig = interpolate(\n",
    "                        orig[None, None, :, :, :],\n",
    "                        size=[img_size, img_size, img_size],\n",
    "                        mode=\"trilinear\",\n",
    "                        align_corners=True,\n",
    "                    )\n",
    "                new_false_grids.append(orig)\n",
    "            new_false_grids = torch.concat(new_false_grids, axis=0)\n",
    "            dict_batch[\"false_grid\"] = new_false_grids\n",
    "\n",
    "        dict_batch.pop(\"grid_data\", None)\n",
    "        dict_batch.pop(\"false_grid_data\", None)\n",
    "        dict_batch.pop(\"cell\", None)\n",
    "        dict_batch.pop(\"false_cell\", None)\n",
    "\n",
    "        return dict_batch\n",
    "\n",
    "\n",
    "# MOFTransformer version 2.0.0\n",
    "import functools\n",
    "from typing import Optional\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from pytorch_lightning import LightningDataModule\n",
    "\n",
    "\n",
    "# Modified datamodule\n",
    "class Datamodule1(LightningDataModule):\n",
    "    def __init__(self, _config):\n",
    "        super().__init__()\n",
    "\n",
    "        self.data_dir = _config[\"root_dataset\"]\n",
    "\n",
    "        self.num_workers = _config[\"num_workers\"]\n",
    "        self.batch_size = _config[\"per_gpu_batchsize\"]\n",
    "        self.eval_batch_size = self.batch_size\n",
    "\n",
    "        self.draw_false_grid = _config[\"draw_false_grid\"]\n",
    "        self.img_size = _config[\"img_size\"]\n",
    "        self.downstream = _config[\"downstream\"]\n",
    "\n",
    "        self.nbr_fea_len = _config[\"nbr_fea_len\"]\n",
    "\n",
    "        self.tasks = [k for k, v in _config[\"loss_names\"].items() if v >= 1]\n",
    "\n",
    "    @property\n",
    "    def dataset_cls(self):\n",
    "        return Dataset1\n",
    "\n",
    "    def set_train_dataset(self):\n",
    "        self.train_dataset = self.dataset_cls(\n",
    "            self.data_dir,\n",
    "            split=\"train\",\n",
    "            draw_false_grid=self.draw_false_grid,\n",
    "            downstream=self.downstream,\n",
    "            nbr_fea_len=self.nbr_fea_len,\n",
    "            tasks=self.tasks,\n",
    "        )\n",
    "\n",
    "    def set_val_dataset(self):\n",
    "        self.val_dataset = self.dataset_cls(\n",
    "            self.data_dir,\n",
    "            split=\"val\",\n",
    "            draw_false_grid=self.draw_false_grid,\n",
    "            downstream=self.downstream,\n",
    "            nbr_fea_len=self.nbr_fea_len,\n",
    "            tasks=self.tasks,\n",
    "        )\n",
    "\n",
    "    def set_test_dataset(self):\n",
    "        self.test_dataset = self.dataset_cls(\n",
    "            self.data_dir,\n",
    "            split=\"test\",\n",
    "            draw_false_grid=self.draw_false_grid,\n",
    "            downstream=self.downstream,\n",
    "            nbr_fea_len=self.nbr_fea_len,\n",
    "            tasks=self.tasks,\n",
    "        )\n",
    "\n",
    "    def setup(self, stage: Optional[str] = None):\n",
    "        if stage in (None, \"fit\"):\n",
    "            self.set_train_dataset()\n",
    "            self.set_val_dataset()\n",
    "\n",
    "        if stage in (None, \"test\"):\n",
    "            self.set_test_dataset()\n",
    "\n",
    "        self.collate = functools.partial(\n",
    "            self.dataset_cls.collate,\n",
    "            img_size=self.img_size,\n",
    "        )\n",
    "\n",
    "    def train_dataloader(self) -> DataLoader:\n",
    "        return DataLoader(\n",
    "            self.train_dataset,\n",
    "            batch_size=self.batch_size,\n",
    "            num_workers=self.num_workers,\n",
    "            collate_fn=self.collate,\n",
    "            shuffle=True,\n",
    "        )\n",
    "\n",
    "    def val_dataloader(self) -> DataLoader:\n",
    "        return DataLoader(\n",
    "            self.val_dataset,\n",
    "            batch_size=self.batch_size,\n",
    "            num_workers=self.num_workers,\n",
    "            collate_fn=self.collate,\n",
    "        )\n",
    "\n",
    "    def test_dataloader(self) -> DataLoader:\n",
    "        return DataLoader(\n",
    "            self.test_dataset,\n",
    "            batch_size=self.batch_size,\n",
    "            num_workers=self.num_workers,\n",
    "            collate_fn=self.collate,\n",
    "        )\n",
    "\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchmetrics.functional import mean_absolute_error\n",
    "\n",
    "\n",
    "def init_weights(module):\n",
    "    if isinstance(module, (nn.Linear, nn.Embedding)):\n",
    "        module.weight.data.normal_(mean=0.0, std=0.02)\n",
    "    elif isinstance(module, nn.LayerNorm):\n",
    "        module.bias.data.zero_()\n",
    "        module.weight.data.fill_(1.0)\n",
    "\n",
    "    if isinstance(module, nn.Linear) and module.bias is not None:\n",
    "        module.bias.data.zero_()\n",
    "\n",
    "\n",
    "# modified loss function with uncertainty logarithm\n",
    "def compute_loss(logits, labels, log_sigma):\n",
    "    loss = 0.5 * torch.exp(-log_sigma) * (labels - logits) ** 2 + 0.5 * log_sigma\n",
    "    return loss.mean()\n",
    "\n",
    "\n",
    "def compute_regression(pl_module, batch, normalizer):\n",
    "    infer = pl_module.infer(batch)\n",
    "\n",
    "    out = pl_module.regression_head(infer[\"cls_feats\"])\n",
    "    logits = out[:, 0]  # [B]\n",
    "    log_sigma = out[:, 1]  # uncertainty logarithm\n",
    "    labels = torch.FloatTensor(batch[\"target\"]).to(logits.device)  # [B]\n",
    "    assert len(labels.shape) == 1\n",
    "\n",
    "    # normalize encode if config[\"mean\"] and config[\"std], else pass\n",
    "    labels = normalizer.encode(labels)\n",
    "    loss = compute_loss(logits, labels, log_sigma)\n",
    "\n",
    "    labels = labels.to(torch.float32)\n",
    "    logits = logits.to(torch.float32)\n",
    "\n",
    "    ret = {\n",
    "        \"cif_id\": infer[\"cif_id\"],\n",
    "        \"cls_feats\": infer[\"cls_feats\"],\n",
    "        \"regression_loss\": loss,\n",
    "        \"regression_logits\": normalizer.decode(logits),\n",
    "        \"regression_labels\": normalizer.decode(labels),\n",
    "        \"log_sigma\": log_sigma,  # uncertainty logarithm\n",
    "    }\n",
    "\n",
    "    # call update() loss and acc\n",
    "    phase = \"train\" if pl_module.training else \"val\"\n",
    "    loss = getattr(pl_module, f\"{phase}_regression_loss\")(ret[\"regression_loss\"])\n",
    "    mae = getattr(pl_module, f\"{phase}_regression_mae\")(\n",
    "        mean_absolute_error(\n",
    "            ret[\"regression_logits\"].cpu(), ret[\"regression_labels\"].cpu()\n",
    "        )\n",
    "    )\n",
    "\n",
    "    if pl_module.write_log:\n",
    "        pl_module.log(f\"regression/{phase}/loss\", loss, sync_dist=True)\n",
    "        pl_module.log(f\"regression/{phase}/mae\", mae, sync_dist=True)\n",
    "\n",
    "    return ret\n",
    "\n",
    "\n",
    "def compute_classification(pl_module, batch):\n",
    "    infer = pl_module.infer(batch)\n",
    "\n",
    "    logits, binary = pl_module.classification_head(\n",
    "        infer[\"cls_feats\"]\n",
    "    )  # [B, output_dim]\n",
    "    labels = torch.LongTensor(batch[\"target\"]).to(logits.device)  # [B]\n",
    "    assert len(labels.shape) == 1\n",
    "    if binary:\n",
    "        logits = logits.squeeze(dim=-1)\n",
    "        loss = F.binary_cross_entropy_with_logits(input=logits, target=labels.float())\n",
    "    else:\n",
    "        loss = F.cross_entropy(logits, labels)\n",
    "\n",
    "    ret = {\n",
    "        \"cif_id\": infer[\"cif_id\"],\n",
    "        \"cls_feats\": infer[\"cls_feats\"],\n",
    "        \"classification_loss\": loss,\n",
    "        \"classification_logits\": logits,\n",
    "        \"classification_labels\": labels,\n",
    "    }\n",
    "\n",
    "    # call update() loss and acc\n",
    "    phase = \"train\" if pl_module.training else \"val\"\n",
    "    loss = getattr(pl_module, f\"{phase}_classification_loss\")(\n",
    "        ret[\"classification_loss\"]\n",
    "    )\n",
    "    acc = getattr(pl_module, f\"{phase}_classification_accuracy\")(\n",
    "        torch.sigmoid(ret[\"classification_logits\"]),\n",
    "        ret[\"classification_labels\"],  # у авторов не было сигмоиды, но она нужна\n",
    "    )\n",
    "\n",
    "    if pl_module.write_log:\n",
    "        pl_module.log(f\"classification/{phase}/loss\", loss, sync_dist=True)\n",
    "        pl_module.log(f\"classification/{phase}/accuracy\", acc, sync_dist=True)\n",
    "\n",
    "    return ret\n",
    "\n",
    "\n",
    "def compute_mpp(pl_module, batch):\n",
    "    infer = pl_module.infer(batch, mask_grid=True)\n",
    "\n",
    "    mpp_logits = pl_module.mpp_head(infer[\"grid_feats\"])  # [B, max_image_len+2, bins]\n",
    "    mpp_logits = mpp_logits[\n",
    "        :, :-1, :\n",
    "    ]  # ignore volume embedding, [B, max_image_len+1, bins]\n",
    "    mpp_labels = infer[\"grid_labels\"]  # [B, max_image_len+1, C=1]\n",
    "\n",
    "    mask = mpp_labels != -100.0  # [B, max_image_len, 1]\n",
    "\n",
    "    # masking\n",
    "    mpp_logits = mpp_logits[mask.squeeze(-1)]  # [mask, bins]\n",
    "    mpp_labels = mpp_labels[mask].long()  # [mask]\n",
    "\n",
    "    mpp_loss = F.cross_entropy(mpp_logits, mpp_labels)\n",
    "\n",
    "    ret = {\n",
    "        \"mpp_loss\": mpp_loss,\n",
    "        \"mpp_logits\": mpp_logits,\n",
    "        \"mpp_labels\": mpp_labels,\n",
    "    }\n",
    "\n",
    "    # call update() loss and acc\n",
    "    phase = \"train\" if pl_module.training else \"val\"\n",
    "    loss = getattr(pl_module, f\"{phase}_mpp_loss\")(ret[\"mpp_loss\"])\n",
    "    acc = getattr(pl_module, f\"{phase}_mpp_accuracy\")(\n",
    "        ret[\"mpp_logits\"], ret[\"mpp_labels\"]\n",
    "    )\n",
    "\n",
    "    if pl_module.write_log:\n",
    "        pl_module.log(f\"mpp/{phase}/loss\", loss, sync_dist=True)\n",
    "        pl_module.log(f\"mpp/{phase}/accuracy\", acc, sync_dist=True)\n",
    "\n",
    "    return ret\n",
    "\n",
    "\n",
    "def compute_mtp(pl_module, batch):\n",
    "    infer = pl_module.infer(batch)\n",
    "    mtp_logits = pl_module.mtp_head(infer[\"cls_feats\"])  # [B, hid_dim]\n",
    "    mtp_labels = torch.LongTensor(batch[\"mtp\"]).to(mtp_logits.device)  # [B]\n",
    "\n",
    "    mtp_loss = F.cross_entropy(mtp_logits, mtp_labels)  # [B]\n",
    "\n",
    "    ret = {\n",
    "        \"mtp_loss\": mtp_loss,\n",
    "        \"mtp_logits\": mtp_logits,\n",
    "        \"mtp_labels\": mtp_labels,\n",
    "    }\n",
    "\n",
    "    # call update() loss and acc\n",
    "    phase = \"train\" if pl_module.training else \"val\"\n",
    "    loss = getattr(pl_module, f\"{phase}_mtp_loss\")(ret[\"mtp_loss\"])\n",
    "    acc = getattr(pl_module, f\"{phase}_mtp_accuracy\")(\n",
    "        ret[\"mtp_logits\"], ret[\"mtp_labels\"]\n",
    "    )\n",
    "\n",
    "    if pl_module.write_log:\n",
    "        pl_module.log(f\"mtp/{phase}/loss\", loss, sync_dist=True)\n",
    "        pl_module.log(f\"mtp/{phase}/accuracy\", acc, sync_dist=True)\n",
    "\n",
    "    return ret\n",
    "\n",
    "\n",
    "def compute_vfp(pl_module, batch):\n",
    "    infer = pl_module.infer(batch)\n",
    "\n",
    "    vfp_logits = pl_module.vfp_head(infer[\"cls_feats\"]).squeeze(-1)  # [B]\n",
    "    vfp_labels = torch.FloatTensor(batch[\"vfp\"]).to(vfp_logits.device)\n",
    "\n",
    "    assert len(vfp_labels.shape) == 1\n",
    "\n",
    "    vfp_loss = F.mse_loss(vfp_logits, vfp_labels)\n",
    "    ret = {\n",
    "        \"vfp_loss\": vfp_loss,\n",
    "        \"vfp_logits\": vfp_logits,\n",
    "        \"vfp_labels\": vfp_labels,\n",
    "    }\n",
    "\n",
    "    # call update() loss and acc\n",
    "    phase = \"train\" if pl_module.training else \"val\"\n",
    "    loss = getattr(pl_module, f\"{phase}_vfp_loss\")(ret[\"vfp_loss\"])\n",
    "    mae = getattr(pl_module, f\"{phase}_vfp_mae\")(\n",
    "        mean_absolute_error(ret[\"vfp_logits\"], ret[\"vfp_labels\"])\n",
    "    )\n",
    "\n",
    "    if pl_module.write_log:\n",
    "        pl_module.log(f\"vfp/{phase}/loss\", loss, sync_dist=True)\n",
    "        pl_module.log(f\"vfp/{phase}/mae\", mae, sync_dist=True)\n",
    "\n",
    "    return ret\n",
    "\n",
    "\n",
    "def compute_ggm(pl_module, batch):\n",
    "    pos_len = len(batch[\"grid\"]) // 2\n",
    "    neg_len = len(batch[\"grid\"]) - pos_len\n",
    "    ggm_labels = torch.cat([torch.ones(pos_len), torch.zeros(neg_len)]).to(\n",
    "        pl_module.device\n",
    "    )\n",
    "\n",
    "    ggm_images = []\n",
    "    for i, (bti, bfi) in enumerate(zip(batch[\"grid\"], batch[\"false_grid\"])):\n",
    "        if ggm_labels[i] == 1:\n",
    "            ggm_images.append(bti)\n",
    "        else:\n",
    "            ggm_images.append(bfi)\n",
    "\n",
    "    ggm_images = torch.stack(ggm_images, dim=0)\n",
    "\n",
    "    batch = {k: v for k, v in batch.items()}\n",
    "    batch[\"grid\"] = ggm_images\n",
    "\n",
    "    infer = pl_module.infer(batch)\n",
    "    ggm_logits = pl_module.ggm_head(infer[\"cls_feats\"])  # cls_feats\n",
    "    ggm_loss = F.cross_entropy(ggm_logits, ggm_labels.long())\n",
    "\n",
    "    ret = {\n",
    "        \"ggm_loss\": ggm_loss,\n",
    "        \"ggm_logits\": ggm_logits,\n",
    "        \"ggm_labels\": ggm_labels,\n",
    "    }\n",
    "\n",
    "    # call update() loss and acc\n",
    "    phase = \"train\" if pl_module.training else \"val\"\n",
    "    loss = getattr(pl_module, f\"{phase}_ggm_loss\")(ret[\"ggm_loss\"])\n",
    "    acc = getattr(pl_module, f\"{phase}_ggm_accuracy\")(\n",
    "        ret[\"ggm_logits\"], ret[\"ggm_labels\"]\n",
    "    )\n",
    "\n",
    "    if pl_module.write_log:\n",
    "        pl_module.log(f\"ggm/{phase}/loss\", loss, sync_dist=True)\n",
    "        pl_module.log(f\"ggm/{phase}/accuracy\", acc, sync_dist=True)\n",
    "\n",
    "    return ret\n",
    "\n",
    "\n",
    "def compute_moc(pl_module, batch):\n",
    "    if \"bbc\" in batch.keys():\n",
    "        task = \"bbc\"\n",
    "    else:\n",
    "        task = \"moc\"\n",
    "\n",
    "    infer = pl_module.infer(batch)\n",
    "    moc_logits = pl_module.moc_head(\n",
    "        infer[\"graph_feats\"][:, 1:, :]\n",
    "    ).flatten()  # [B, max_graph_len] -> [B * max_graph_len]\n",
    "    moc_labels = (\n",
    "        infer[\"mo_labels\"].to(moc_logits).flatten()\n",
    "    )  # [B, max_graph_len] -> [B * max_graph_len]\n",
    "    mask = moc_labels != -100\n",
    "\n",
    "    moc_loss = F.binary_cross_entropy_with_logits(\n",
    "        input=moc_logits[mask], target=moc_labels[mask]\n",
    "    )  # [B * max_graph_len]\n",
    "\n",
    "    ret = {\n",
    "        \"moc_loss\": moc_loss,\n",
    "        \"moc_logits\": moc_logits,\n",
    "        \"moc_labels\": moc_labels,\n",
    "    }\n",
    "\n",
    "    # call update() loss and acc\n",
    "    phase = \"train\" if pl_module.training else \"val\"\n",
    "    loss = getattr(pl_module, f\"{phase}_{task}_loss\")(ret[\"moc_loss\"])\n",
    "    acc = getattr(pl_module, f\"{phase}_{task}_accuracy\")(\n",
    "        nn.Sigmoid()(ret[\"moc_logits\"]), ret[\"moc_labels\"].long()\n",
    "    )\n",
    "\n",
    "    if pl_module.write_log:\n",
    "        pl_module.log(f\"{task}/{phase}/loss\", loss, sync_dist=True)\n",
    "        pl_module.log(f\"{task}/{phase}/accuracy\", acc, sync_dist=True)\n",
    "\n",
    "    return ret\n",
    "\n",
    "\n",
    "# Regression head\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class RegressionHead(nn.Module):\n",
    "    \"\"\"\n",
    "    Modified head for Regression\n",
    "    Original: self.layer = nn.Linear(hid_dim, 1)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, hid_dim):\n",
    "        super().__init__()\n",
    "        self.layer = nn.Sequential(\n",
    "            nn.Linear(hid_dim, 2048), nn.ReLU(), nn.Linear(2048, 2)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.layer(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "# Classification head (original MOFTransformer)\n",
    "class ClassificationHead(nn.Module):\n",
    "    \"\"\"\n",
    "    head for Classification\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, hid_dim, n_classes):\n",
    "        super().__init__()\n",
    "\n",
    "        if n_classes == 2:\n",
    "            self.layer = nn.Sequential(\n",
    "                nn.Linear(hid_dim, 1),\n",
    "            )\n",
    "            self.binary = True\n",
    "        else:\n",
    "            self.layer = nn.Sequential(nn.Linear(hid_dim, 1))\n",
    "            self.binary = False\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.layer(x)\n",
    "\n",
    "        return x, self.binary\n",
    "\n",
    "\n",
    "# MOFTransformer version 2.1.0\n",
    "from typing import Any, List\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning import LightningModule\n",
    "\n",
    "from moftransformer.modules import objectives, heads, module_utils\n",
    "from moftransformer.modules.cgcnn import GraphEmbeddings\n",
    "from moftransformer.modules.vision_transformer_3d import VisionTransformer3D\n",
    "\n",
    "from moftransformer.modules.module_utils import Normalizer\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "\n",
    "# Modified model\n",
    "class Module1(LightningModule):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "\n",
    "        self.max_grid_len = config[\"max_grid_len\"]\n",
    "        self.vis = config[\"visualize\"]\n",
    "\n",
    "        # graph embedding with_unique_atoms\n",
    "        self.graph_embeddings = GraphEmbeddings(\n",
    "            atom_fea_len=config[\"atom_fea_len\"],\n",
    "            nbr_fea_len=config[\"nbr_fea_len\"],\n",
    "            max_graph_len=config[\"max_graph_len\"],\n",
    "            hid_dim=config[\"hid_dim\"],\n",
    "            vis=config[\"visualize\"],\n",
    "            n_conv=3,\n",
    "        )\n",
    "        self.graph_embeddings.apply(objectives.init_weights)\n",
    "\n",
    "        # token type embeddings\n",
    "        self.token_type_embeddings = nn.Embedding(2, config[\"hid_dim\"])\n",
    "        self.token_type_embeddings.apply(objectives.init_weights)\n",
    "\n",
    "        # set transformer\n",
    "        self.transformer = VisionTransformer3D(\n",
    "            img_size=config[\"img_size\"],\n",
    "            patch_size=config[\"patch_size\"],\n",
    "            in_chans=config[\"in_chans\"],\n",
    "            embed_dim=config[\"hid_dim\"],\n",
    "            depth=config[\"num_layers\"],\n",
    "            num_heads=config[\"num_heads\"],\n",
    "            mlp_ratio=config[\"mlp_ratio\"],\n",
    "            drop_rate=config[\"drop_rate\"],\n",
    "            mpp_ratio=config[\"mpp_ratio\"],\n",
    "        )\n",
    "\n",
    "        # class token\n",
    "        self.cls_embeddings = nn.Linear(1, config[\"hid_dim\"])\n",
    "        self.cls_embeddings.apply(objectives.init_weights)\n",
    "\n",
    "        # volume token\n",
    "        self.volume_embeddings = nn.Linear(1, config[\"hid_dim\"])\n",
    "        self.volume_embeddings.apply(objectives.init_weights)\n",
    "\n",
    "        # add features tokens\n",
    "        self.add_features_embeddings = nn.Linear(\n",
    "            N_ADD_FEATURES, config[\"hid_dim\"]\n",
    "        )  # linear embedding for add features\n",
    "        self.add_features_embeddings.apply(objectives.init_weights)\n",
    "\n",
    "        self.pooler = heads.Pooler(config[\"hid_dim\"])\n",
    "        self.pooler.apply(objectives.init_weights)\n",
    "\n",
    "        # ===================== loss =====================\n",
    "        if config[\"loss_names\"][\"ggm\"] > 0:\n",
    "            self.ggm_head = heads.GGMHead(config[\"hid_dim\"])\n",
    "            self.ggm_head.apply(objectives.init_weights)\n",
    "\n",
    "        if config[\"loss_names\"][\"mpp\"] > 0:\n",
    "            self.mpp_head = heads.MPPHead(config[\"hid_dim\"])\n",
    "            self.mpp_head.apply(objectives.init_weights)\n",
    "\n",
    "        if config[\"loss_names\"][\"mtp\"] > 0:\n",
    "            self.mtp_head = heads.MTPHead(config[\"hid_dim\"])\n",
    "            self.mtp_head.apply(objectives.init_weights)\n",
    "\n",
    "        if config[\"loss_names\"][\"vfp\"] > 0:\n",
    "            self.vfp_head = heads.VFPHead(config[\"hid_dim\"])\n",
    "            self.vfp_head.apply(objectives.init_weights)\n",
    "\n",
    "        if config[\"loss_names\"][\"moc\"] > 0 or config[\"loss_names\"][\"bbc\"] > 0:\n",
    "            self.moc_head = heads.MOCHead(config[\"hid_dim\"])\n",
    "            self.moc_head.apply(objectives.init_weights)\n",
    "\n",
    "        # ===================== Downstream =====================\n",
    "        hid_dim = config[\"hid_dim\"]\n",
    "\n",
    "        if config[\"load_path\"] != \"\" and not config[\"test_only\"]:\n",
    "            ckpt = torch.load(self.hparams.config[\"load_path\"], map_location=\"cpu\")\n",
    "            state_dict = ckpt[\"state_dict\"]\n",
    "            self.load_state_dict(state_dict, strict=False)\n",
    "            print(f\"load model : {config['load_path']}\")\n",
    "\n",
    "        if self.hparams.config[\"loss_names\"][\"regression\"] > 0:\n",
    "            self.regression_head = RegressionHead(hid_dim)\n",
    "            self.regression_head.apply(objectives.init_weights)\n",
    "            # normalization\n",
    "            self.mean = config[\"mean\"]\n",
    "            self.std = config[\"std\"]\n",
    "\n",
    "        if self.hparams.config[\"loss_names\"][\"classification\"] > 0:\n",
    "            n_classes = config[\"n_classes\"]\n",
    "            self.classification_head = heads.ClassificationHead(hid_dim, n_classes)\n",
    "            self.classification_head.apply(objectives.init_weights)\n",
    "\n",
    "        module_utils.set_metrics(self)\n",
    "        self.current_tasks = list()\n",
    "        # ===================== load downstream (test_only) ======================\n",
    "\n",
    "        if config[\"load_path\"] != \"\" and config[\"test_only\"]:\n",
    "            ckpt = torch.load(config[\"load_path\"], map_location=\"cpu\")\n",
    "            state_dict = ckpt[\"state_dict\"]\n",
    "            self.load_state_dict(state_dict, strict=False)\n",
    "            print(f\"load model : {config['load_path']}\")\n",
    "\n",
    "        self.test_logits = []\n",
    "        self.test_labels = []\n",
    "        self.test_cifid = []\n",
    "        self.write_log = True\n",
    "\n",
    "    def infer(\n",
    "        self,\n",
    "        batch,\n",
    "        mask_grid=False,\n",
    "    ):\n",
    "        cif_id = batch[\"cif_id\"]\n",
    "        atom_num = batch[\"atom_num\"]  # [N']\n",
    "        nbr_idx = batch[\"nbr_idx\"]  # [N', M]\n",
    "        nbr_fea = batch[\"nbr_fea\"]  # [N', M, nbr_fea_len]\n",
    "        crystal_atom_idx = batch[\"crystal_atom_idx\"]  # list [B]\n",
    "        uni_idx = batch[\"uni_idx\"]  # list [B]\n",
    "        uni_count = batch[\"uni_count\"]  # list [B]\n",
    "\n",
    "        grid = batch[\"grid\"]  # [B, C, H, W, D]\n",
    "        volume = batch[\"volume\"]  # list [B]\n",
    "        add_features = batch[\"add_features\"]  # add features [B, N_ADD_FEATURES]\n",
    "\n",
    "        if \"moc\" in batch.keys():\n",
    "            moc = batch[\"moc\"]  # [B]\n",
    "        elif \"bbc\" in batch.keys():\n",
    "            moc = batch[\"bbc\"]  # [B]\n",
    "        else:\n",
    "            moc = None\n",
    "\n",
    "        # get graph embeds\n",
    "        (\n",
    "            graph_embeds,  # [B, max_graph_len, hid_dim],\n",
    "            graph_masks,  # [B, max_graph_len],\n",
    "            mo_labels,  # if moc: [B, max_graph_len], else: None\n",
    "        ) = self.graph_embeddings(\n",
    "            atom_num=atom_num,\n",
    "            nbr_idx=nbr_idx,\n",
    "            nbr_fea=nbr_fea,\n",
    "            crystal_atom_idx=crystal_atom_idx,\n",
    "            uni_idx=uni_idx,\n",
    "            uni_count=uni_count,\n",
    "            moc=moc,\n",
    "        )\n",
    "        # add class embeds to graph_embeds\n",
    "        cls_tokens = torch.zeros(len(crystal_atom_idx)).to(graph_embeds)  # [B]\n",
    "        cls_embeds = self.cls_embeddings(cls_tokens[:, None, None])  # [B, 1, hid_dim]\n",
    "        cls_mask = torch.ones(len(crystal_atom_idx), 1).to(graph_masks)  # [B, 1]\n",
    "\n",
    "        graph_embeds = torch.cat(\n",
    "            [cls_embeds, graph_embeds], dim=1\n",
    "        )  # [B, max_graph_len+1, hid_dim]\n",
    "        graph_masks = torch.cat([cls_mask, graph_masks], dim=1)  # [B, max_graph_len+1]\n",
    "\n",
    "        # get grid embeds\n",
    "        (\n",
    "            grid_embeds,  # [B, max_grid_len+1, hid_dim]\n",
    "            grid_masks,  # [B, max_grid_len+1]\n",
    "            grid_labels,  # [B, grid+1, C] if mask_image == True\n",
    "        ) = self.transformer.visual_embed(\n",
    "            grid,\n",
    "            max_image_len=self.max_grid_len,\n",
    "            mask_it=mask_grid,\n",
    "        )\n",
    "\n",
    "        # add volume embeds to grid_embeds\n",
    "        volume = torch.FloatTensor(volume).to(grid_embeds)  # [B]\n",
    "        volume_embeds = self.volume_embeddings(volume[:, None, None])  # [B, 1, hid_dim]\n",
    "        volume_mask = torch.ones(volume.shape[0], 1).to(grid_masks)\n",
    "        # add add_features embeds to grid_embeds\n",
    "        add_features = torch.FloatTensor(add_features).to(grid_embeds)  # [B]\n",
    "        add_features_embeds = self.add_features_embeddings(\n",
    "            add_features[:, None]\n",
    "        )  # [B, 1, hid_dim]\n",
    "        add_features_mask = torch.ones(add_features.shape[0], 1).to(grid_masks)\n",
    "        grid_embeds = torch.cat(\n",
    "            [grid_embeds, volume_embeds, add_features_embeds], dim=1\n",
    "        )  # [B, max_grid_len+2, hid_dim]\n",
    "        grid_masks = torch.cat(\n",
    "            [grid_masks, volume_mask, add_features_mask], dim=1\n",
    "        )  # [B, max_grid_len+2]\n",
    "\n",
    "        # add token_type_embeddings\n",
    "        graph_embeds = graph_embeds + self.token_type_embeddings(\n",
    "            torch.zeros_like(graph_masks, device=self.device).long()\n",
    "        )\n",
    "        grid_embeds = grid_embeds + self.token_type_embeddings(\n",
    "            torch.ones_like(grid_masks, device=self.device).long()\n",
    "        )\n",
    "\n",
    "        co_embeds = torch.cat(\n",
    "            [graph_embeds, grid_embeds], dim=1\n",
    "        )  # [B, final_max_len, hid_dim]\n",
    "        co_masks = torch.cat(\n",
    "            [graph_masks, grid_masks], dim=1\n",
    "        )  # [B, final_max_len, hid_dim]\n",
    "\n",
    "        x = co_embeds\n",
    "\n",
    "        attn_weights = []\n",
    "        for i, blk in enumerate(self.transformer.blocks):\n",
    "            x, _attn = blk(x, mask=co_masks)\n",
    "\n",
    "            if self.vis:\n",
    "                attn_weights.append(_attn)\n",
    "\n",
    "        x = self.transformer.norm(x)\n",
    "        graph_feats, grid_feats = (\n",
    "            x[:, : graph_embeds.shape[1]],\n",
    "            x[:, graph_embeds.shape[1] :],\n",
    "        )  # [B, max_graph_len, hid_dim], [B, max_grid_len+2, hid_dim]\n",
    "\n",
    "        cls_feats = self.pooler(x)  # [B, hid_dim]\n",
    "\n",
    "        ret = {\n",
    "            \"graph_feats\": graph_feats,\n",
    "            \"grid_feats\": grid_feats,\n",
    "            \"cls_feats\": cls_feats,\n",
    "            \"raw_cls_feats\": x[:, 0],\n",
    "            \"graph_masks\": graph_masks,\n",
    "            \"grid_masks\": grid_masks,\n",
    "            \"grid_labels\": grid_labels,  # if MPP, else None\n",
    "            \"mo_labels\": mo_labels,  # if MOC, else None\n",
    "            \"cif_id\": cif_id,\n",
    "            \"attn_weights\": attn_weights,\n",
    "            \"add_features\": torch.tensor(batch[\"add_features\"]).to(\n",
    "                cls_feats\n",
    "            ),  # add features\n",
    "        }\n",
    "\n",
    "        return ret\n",
    "\n",
    "    def forward(self, batch):\n",
    "        ret = dict()\n",
    "\n",
    "        if len(self.current_tasks) == 0:\n",
    "            ret.update(self.infer(batch))\n",
    "            return ret\n",
    "\n",
    "        # Masked Patch Prediction\n",
    "        if \"mpp\" in self.current_tasks:\n",
    "            ret.update(objectives.compute_mpp(self, batch))\n",
    "\n",
    "        # Graph Grid Matching\n",
    "        if \"ggm\" in self.current_tasks:\n",
    "            ret.update(objectives.compute_ggm(self, batch))\n",
    "\n",
    "        # MOF Topology Prediction\n",
    "        if \"mtp\" in self.current_tasks:\n",
    "            ret.update(objectives.compute_mtp(self, batch))\n",
    "\n",
    "        # Void Fraction Prediction\n",
    "        if \"vfp\" in self.current_tasks:\n",
    "            ret.update(objectives.compute_vfp(self, batch))\n",
    "\n",
    "        # Metal Organic Classification (or Building Block Classfication)\n",
    "        if \"moc\" in self.current_tasks or \"bbc\" in self.current_tasks:\n",
    "            ret.update(objectives.compute_moc(self, batch))\n",
    "\n",
    "        # regression\n",
    "        if \"regression\" in self.current_tasks:\n",
    "            normalizer = Normalizer(self.mean, self.std)\n",
    "            # ret.update(objectives.compute_regression(self, batch, normalizer))\n",
    "            ret.update(compute_regression(self, batch, normalizer))\n",
    "\n",
    "        # classification\n",
    "        if \"classification\" in self.current_tasks:\n",
    "            # ret.update(objectives.compute_classification(self, batch))\n",
    "            ret.update(compute_classification(self, batch))\n",
    "        return ret\n",
    "\n",
    "    def on_train_start(self):\n",
    "        module_utils.set_task(self)\n",
    "        self.write_log = True\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        output = self(batch)\n",
    "        total_loss = sum([v for k, v in output.items() if \"loss\" in k])\n",
    "        return total_loss\n",
    "\n",
    "    def on_train_epoch_end(self):\n",
    "        module_utils.epoch_wrapup(self)\n",
    "\n",
    "    def on_validation_start(self):\n",
    "        module_utils.set_task(self)\n",
    "        self.write_log = True\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        output = self(batch)\n",
    "\n",
    "    def on_validation_epoch_end(self) -> None:\n",
    "        module_utils.epoch_wrapup(self)\n",
    "\n",
    "    def on_test_start(\n",
    "        self,\n",
    "    ):\n",
    "        module_utils.set_task(self)\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        output = self(batch)\n",
    "        output = {\n",
    "            k: (v.cpu() if torch.is_tensor(v) else v) for k, v in output.items()\n",
    "        }  # update cpu for memory\n",
    "\n",
    "        if \"regression_logits\" in output.keys():\n",
    "            self.test_logits += output[\"regression_logits\"].tolist()\n",
    "            self.test_labels += output[\"regression_labels\"].tolist()\n",
    "        return output\n",
    "\n",
    "    def on_test_epoch_end(self):\n",
    "        module_utils.epoch_wrapup(self)\n",
    "\n",
    "        # calculate r2 score when regression\n",
    "        if len(self.test_logits) > 1:\n",
    "            r2 = r2_score(np.array(self.test_labels), np.array(self.test_logits))\n",
    "            self.log(f\"test/r2_score\", r2, sync_dist=True)\n",
    "            self.test_labels.clear()\n",
    "            self.test_logits.clear()\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return module_utils.set_schedule(self)\n",
    "\n",
    "    def on_predict_start(self):\n",
    "        self.write_log = False\n",
    "        module_utils.set_task(self)\n",
    "\n",
    "    def predict_step(self, batch, batch_idx, dataloader_idx=0):\n",
    "        output = self(batch)\n",
    "\n",
    "        if \"classification_logits\" in output:\n",
    "            if self.hparams.config[\"n_classes\"] == 2:\n",
    "                output[\"classification_logits_index\"] = torch.round(\n",
    "                    torch.sigmoid(output[\"classification_logits\"])\n",
    "                ).to(\n",
    "                    torch.int\n",
    "                )  # added sigmoid\n",
    "            else:\n",
    "                softmax = torch.nn.Softmax(dim=1)\n",
    "                output[\"classification_logits\"] = softmax(\n",
    "                    output[\"classification_logits\"]\n",
    "                )\n",
    "                output[\"classification_logits_index\"] = torch.argmax(\n",
    "                    output[\"classification_logits\"], dim=1\n",
    "                )\n",
    "\n",
    "        output = {\n",
    "            k: (v.cpu().tolist() if torch.is_tensor(v) else v)\n",
    "            for k, v in output.items()\n",
    "            if (\"logits\" in k)\n",
    "            or (\"labels\" in k)\n",
    "            or (\"sigma\" in k)\n",
    "            or \"cif_id\" == k  # added output of sigma\n",
    "        }\n",
    "\n",
    "        return output\n",
    "\n",
    "    def on_predict_epoch_end(self, *args):\n",
    "        self.test_labels.clear()\n",
    "        self.test_logits.clear()\n",
    "\n",
    "    def on_predict_end(\n",
    "        self,\n",
    "    ):\n",
    "        self.write_log = True\n",
    "\n",
    "    def lr_scheduler_step(self, scheduler, *args):\n",
    "        if len(args) == 2:\n",
    "            optimizer_idx, metric = args\n",
    "        elif len(args) == 1:\n",
    "            (metric,) = args\n",
    "        else:\n",
    "            raise ValueError(\n",
    "                \"lr_scheduler_step must have metric and optimizer_idx(optional)\"\n",
    "            )\n",
    "\n",
    "        if pl.__version__ >= \"2.0.0\":\n",
    "            scheduler.step(epoch=self.current_epoch)\n",
    "        else:\n",
    "            scheduler.step()\n",
    "\n",
    "\n",
    "import sys\n",
    "import os\n",
    "import copy\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "import shutil\n",
    "\n",
    "import pytorch_lightning as pl\n",
    "\n",
    "from moftransformer.config import ex\n",
    "from moftransformer.config import config as _config\n",
    "from moftransformer.utils.validation import (\n",
    "    get_valid_config,\n",
    "    get_num_devices,\n",
    "    ConfigurationError,\n",
    ")\n",
    "\n",
    "warnings.filterwarnings(\n",
    "    \"ignore\", \".*Trying to infer the `batch_size` from an ambiguous collection.*\"\n",
    ")\n",
    "\n",
    "\n",
    "_IS_INTERACTIVE = hasattr(sys, \"ps1\")\n",
    "\n",
    "\n",
    "def run(root_dataset, downstream=None, log_dir=\"logs/\", *, test_only=False, **kwargs):\n",
    "\n",
    "    config = copy.deepcopy(_config())\n",
    "    for key in kwargs.keys():\n",
    "        if key not in config:\n",
    "            raise ConfigurationError(f\"{key} is not in configuration.\")\n",
    "\n",
    "    config.update(kwargs)\n",
    "    config[\"root_dataset\"] = root_dataset\n",
    "    config[\"downstream\"] = downstream\n",
    "    config[\"log_dir\"] = log_dir\n",
    "    config[\"test_only\"] = test_only\n",
    "\n",
    "    main1(config)\n",
    "\n",
    "\n",
    "# @ex.automain\n",
    "def main1(_config):\n",
    "    _config = copy.deepcopy(_config)\n",
    "    pl.seed_everything(_config[\"seed\"])\n",
    "\n",
    "    _config = get_valid_config(_config)\n",
    "    dm = Datamodule1(_config)\n",
    "    model = Module1(_config)\n",
    "\n",
    "    exp_name = f\"{_config['exp_name']}\"\n",
    "\n",
    "    os.makedirs(_config[\"log_dir\"], exist_ok=True)\n",
    "    checkpoint_callback = pl.callbacks.ModelCheckpoint(\n",
    "        save_top_k=1,\n",
    "        verbose=True,\n",
    "        monitor=\"val/the_metric\",\n",
    "        mode=\"max\",\n",
    "        save_last=True,\n",
    "    )\n",
    "\n",
    "    if _config[\"test_only\"]:\n",
    "        name = f'test_{exp_name}_seed{_config[\"seed\"]}_from_{str(_config[\"load_path\"]).split(\"/\")[-1][:-5]}'\n",
    "    else:\n",
    "        name = f'{exp_name}_seed{_config[\"seed\"]}_from_{str(_config[\"load_path\"]).split(\"/\")[-1][:-5]}'\n",
    "\n",
    "    logger = pl.loggers.TensorBoardLogger(\n",
    "        _config[\"log_dir\"],\n",
    "        name=name,\n",
    "    )\n",
    "\n",
    "    lr_callback = pl.callbacks.LearningRateMonitor(logging_interval=\"step\")\n",
    "    callbacks = [checkpoint_callback, lr_callback]\n",
    "\n",
    "    num_device = get_num_devices(_config)\n",
    "    print(\"num_device\", num_device)\n",
    "\n",
    "    # gradient accumulation\n",
    "    if num_device == 0:\n",
    "        accumulate_grad_batches = _config[\"batch_size\"] // (\n",
    "            _config[\"per_gpu_batchsize\"] * _config[\"num_nodes\"]\n",
    "        )\n",
    "    else:\n",
    "        accumulate_grad_batches = _config[\"batch_size\"] // (\n",
    "            _config[\"per_gpu_batchsize\"] * num_device * _config[\"num_nodes\"]\n",
    "        )\n",
    "\n",
    "    max_steps = _config[\"max_steps\"] if _config[\"max_steps\"] is not None else None\n",
    "\n",
    "    if _IS_INTERACTIVE:\n",
    "        strategy = None\n",
    "    elif pl.__version__ >= \"2.0.0\":\n",
    "        strategy = \"ddp_find_unused_parameters_true\"\n",
    "    else:\n",
    "        strategy = \"ddp\"\n",
    "\n",
    "    log_every_n_steps = 10\n",
    "\n",
    "    trainer = pl.Trainer(\n",
    "        accelerator=_config[\"accelerator\"],\n",
    "        devices=_config[\"devices\"],\n",
    "        num_nodes=_config[\"num_nodes\"],\n",
    "        precision=_config[\"precision\"],\n",
    "        strategy=strategy,\n",
    "        benchmark=True,\n",
    "        max_epochs=_config[\"max_epochs\"],\n",
    "        max_steps=max_steps,\n",
    "        callbacks=callbacks,\n",
    "        logger=logger,\n",
    "        accumulate_grad_batches=accumulate_grad_batches,\n",
    "        log_every_n_steps=log_every_n_steps,\n",
    "        val_check_interval=_config[\"val_check_interval\"],\n",
    "        deterministic=True,\n",
    "    )\n",
    "\n",
    "    if not _config[\"test_only\"]:\n",
    "        trainer.fit(model, datamodule=dm, ckpt_path=_config[\"resume_from\"])\n",
    "        log_dir = Path(logger.log_dir) / \"checkpoints\"\n",
    "        if best_model := next(log_dir.glob(\"epoch=*.ckpt\")):\n",
    "            shutil.copy(best_model, log_dir / \"best.ckpt\")\n",
    "\n",
    "    else:\n",
    "        trainer.test(model, datamodule=dm)\n",
    "\n",
    "\n",
    "import sys\n",
    "import os\n",
    "import copy\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "import re\n",
    "import csv\n",
    "\n",
    "import pytorch_lightning as pl\n",
    "\n",
    "from moftransformer.config import ex\n",
    "from moftransformer.config import config as _config\n",
    "from moftransformer.modules.module_utils import set_task\n",
    "from moftransformer.utils.validation import (\n",
    "    get_valid_config,\n",
    "    get_num_devices,\n",
    "    ConfigurationError,\n",
    "    _IS_INTERACTIVE,\n",
    ")\n",
    "\n",
    "warnings.filterwarnings(\n",
    "    \"ignore\", \".*Trying to infer the `batch_size` from an ambiguous collection.*\"\n",
    ")\n",
    "\n",
    "\n",
    "def predict(\n",
    "    root_dataset, load_path, downstream=None, split=\"all\", save_dir=None, **kwargs\n",
    "):\n",
    "\n",
    "    config = copy.deepcopy(_config())\n",
    "    for key in kwargs.keys():\n",
    "        if key not in config:\n",
    "            raise ConfigurationError(f\"{key} is not in configuration.\")\n",
    "\n",
    "    config.update(kwargs)\n",
    "    config[\"root_dataset\"] = root_dataset\n",
    "    config[\"downstream\"] = downstream\n",
    "    config[\"load_path\"] = load_path\n",
    "    config[\"test_only\"] = True\n",
    "    config[\"visualize\"] = False\n",
    "    config[\"split\"] = split\n",
    "    config[\"save_dir\"] = save_dir\n",
    "\n",
    "    return main_(config)\n",
    "\n",
    "\n",
    "# @ex.automain\n",
    "def main_(_config):\n",
    "    config = copy.deepcopy(_config)\n",
    "\n",
    "    config[\"test_only\"] = True\n",
    "    config[\"visualize\"] = False\n",
    "\n",
    "    os.makedirs(config[\"log_dir\"], exist_ok=True)\n",
    "    pl.seed_everything(config[\"seed\"])\n",
    "\n",
    "    num_device = get_num_devices(config)\n",
    "    num_nodes = config[\"num_nodes\"]\n",
    "    if num_nodes > 1:\n",
    "        warnings.warn(\n",
    "            f\"function <predict> only support 1 devices. change num_nodes {num_nodes} -> 1\"\n",
    "        )\n",
    "        config[\"num_nodes\"] = 1\n",
    "    if num_device > 1:\n",
    "        warnings.warn(\n",
    "            f\"function <predict> only support 1 devices. change num_devices {num_device} -> 1\"\n",
    "        )\n",
    "        config[\"devices\"] = 1\n",
    "\n",
    "    config = get_valid_config(config)  # valid config\n",
    "    model = Module1(config)\n",
    "    dm = Datamodule1(config)\n",
    "    model.eval()\n",
    "\n",
    "    if _IS_INTERACTIVE:\n",
    "        strategy = None\n",
    "    elif pl.__version__ >= \"2.0.0\":\n",
    "        strategy = \"ddp_find_unused_parameters_true\"\n",
    "    else:\n",
    "        strategy = \"ddp\"\n",
    "\n",
    "    trainer = pl.Trainer(\n",
    "        accelerator=config[\"accelerator\"],\n",
    "        devices=config[\"devices\"],\n",
    "        num_nodes=config[\"num_nodes\"],\n",
    "        precision=config[\"precision\"],\n",
    "        strategy=strategy,\n",
    "        benchmark=True,\n",
    "        max_epochs=1,\n",
    "        log_every_n_steps=0,\n",
    "        deterministic=True,\n",
    "        logger=False,\n",
    "    )\n",
    "\n",
    "    # refine split\n",
    "    split = config.get(\"split\", \"all\")\n",
    "    if split == \"all\":\n",
    "        split = [\"train\", \"val\", \"test\"]\n",
    "    elif isinstance(split, str):\n",
    "        split = re.split(r\",\\s?\", split)\n",
    "\n",
    "    if split == [\"test\"]:\n",
    "        dm.setup(\"test\")\n",
    "    elif \"test\" not in split:\n",
    "        dm.setup(\"fit\")\n",
    "    else:\n",
    "        dm.setup()\n",
    "\n",
    "    # save_dir\n",
    "    save_dir = config.get(\"save_dir\", None)\n",
    "    if save_dir is None:\n",
    "        save_dir = Path(config[\"load_path\"]).parent.parent\n",
    "    else:\n",
    "        save_dir = Path(save_dir)\n",
    "        save_dir.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "    # predict\n",
    "    for s in split:\n",
    "        if not s in [\"train\", \"test\", \"val\"]:\n",
    "            raise ValueError(f\"split must be train, test, or val, not {s}\")\n",
    "\n",
    "        savefile = save_dir / f\"{s}_prediction.csv\"\n",
    "        dataloader = getattr(dm, f\"{s}_dataloader\")()\n",
    "        rets = trainer.predict(model, dataloader)\n",
    "        write_output(rets, savefile)\n",
    "\n",
    "    print(f\"All prediction values are saved in {save_dir}\")\n",
    "    return rets\n",
    "\n",
    "\n",
    "def write_output(rets, savefile):\n",
    "    keys = rets[0].keys()\n",
    "\n",
    "    with open(savefile, \"w\") as f:\n",
    "        wr = csv.writer(f)\n",
    "        wr.writerow(keys)\n",
    "        for ret in rets:\n",
    "            if ret.keys() != keys:\n",
    "                raise ValueError(ret.keys(), keys)\n",
    "\n",
    "            for data in zip(*ret.values()):\n",
    "                wr.writerow(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66e4c21f-560d-483a-b7da-5815f01ebd4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Regression task proton conductivity\n",
    "def main():\n",
    "    for seed in range(10):\n",
    "        for i in range(5):\n",
    "            load_path = \"./path/to/model/weights\"\n",
    "            save_dir = os.path.join(root_dataset, \"results/regression\")\n",
    "            preds = predict(\n",
    "                root_dataset=root_dataset,\n",
    "                load_path=load_path,\n",
    "                downstream=\"pcond\",\n",
    "                split=[\"test\"],\n",
    "                save_dir=save_dir,\n",
    "            )\n",
    "            os.rename(\n",
    "                os.path.join(save_dir, \"test_prediction.csv\"),\n",
    "                os.path.join(save_dir, f\"test_prediction_fold_{i}_seed_{seed}.csv\"),\n",
    "            )\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6a0d1a2d-99f4-4786-bdcc-d34efa5d8f86",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import copy\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "import shutil\n",
    "\n",
    "import pytorch_lightning as pl\n",
    "\n",
    "from moftransformer.config import ex\n",
    "from moftransformer.config import config as _config\n",
    "from moftransformer.utils.validation import (\n",
    "    get_valid_config,\n",
    "    get_num_devices,\n",
    "    ConfigurationError,\n",
    ")\n",
    "\n",
    "warnings.filterwarnings(\n",
    "    \"ignore\", \".*Trying to infer the `batch_size` from an ambiguous collection.*\"\n",
    ")\n",
    "\n",
    "import os\n",
    "import random\n",
    "import json\n",
    "import pickle\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "from torch.nn.functional import interpolate\n",
    "\n",
    "\n",
    "# for classification task\n",
    "N_ADD_FEATURES = 1\n",
    "\n",
    "\n",
    "# Modified dataset\n",
    "class Dataset1(torch.utils.data.Dataset):\n",
    "    def __init__(\n",
    "        self,\n",
    "        data_dir: str,\n",
    "        split: str,\n",
    "        nbr_fea_len: int,\n",
    "        draw_false_grid=True,\n",
    "        downstream=\"\",\n",
    "        tasks=[],\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Dataset for pretrained MOF.\n",
    "        Args:\n",
    "            data_dir (str): where dataset cif files and energy grid file; exist via model.utils.prepare_data.py\n",
    "            split(str) : train, test, split\n",
    "            draw_false_grid (int, optional):  how many generating false_grid_data\n",
    "            nbr_fea_len (int) : nbr_fea_len for gaussian expansion\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.data_dir = data_dir\n",
    "        self.draw_false_grid = draw_false_grid\n",
    "        self.split = split\n",
    "\n",
    "        assert split in {\"train\", \"test\", \"val\"}\n",
    "        if downstream:\n",
    "            path_file = os.path.join(data_dir, f\"{split}_{downstream}.json\")\n",
    "            path_file_features = os.path.join(\n",
    "                data_dir, f\"{split}_{downstream}_features.json\"\n",
    "            )  # path to file with additional (global) features\n",
    "        else:\n",
    "            path_file = os.path.join(data_dir, f\"{split}.json\")\n",
    "            path_file_features = os.path.join(\n",
    "                data_dir, f\"{split}_features.json\"\n",
    "            )  # path to file with additional features\n",
    "        print(f\"read {path_file}...\")\n",
    "        print(f\"read {path_file_features}...\")\n",
    "\n",
    "        if not os.path.isfile(path_file):\n",
    "            raise FileNotFoundError(\n",
    "                f\"{path_file} doesn't exist. Check 'root_dataset' in config\"\n",
    "            )\n",
    "        # if not os.path.isfile(path_file_features):\n",
    "        # self.with_add_features = False #flag for presence of add features\n",
    "        # print('file with additional features does not exist')\n",
    "        # else:\n",
    "        # self.with_add_features = True #flag for presence of add features\n",
    "\n",
    "        dict_target = json.load(open(path_file, \"r\"))\n",
    "        self.cif_ids, self.targets = zip(*dict_target.items())\n",
    "\n",
    "        dict_add_features = json.load(\n",
    "            open(path_file_features, \"r\")\n",
    "        )  # dict with add features\n",
    "        self.add_features = [dict_add_features[cif_id] for cif_id in self.cif_ids]\n",
    "\n",
    "        self.nbr_fea_len = nbr_fea_len\n",
    "\n",
    "        self.tasks = {}\n",
    "\n",
    "        for task in tasks:\n",
    "            if task in [\"mtp\", \"vfp\", \"moc\", \"bbc\"]:\n",
    "                path_file = os.path.join(data_dir, f\"{split}_{task}.json\")\n",
    "                print(f\"read {path_file}...\")\n",
    "                assert os.path.isfile(\n",
    "                    path_file\n",
    "                ), f\"{path_file} doesn't exist in {data_dir}\"\n",
    "\n",
    "                dict_task = json.load(open(path_file, \"r\"))\n",
    "                cif_ids, t = zip(*dict_task.items())\n",
    "                self.tasks[task] = list(t)\n",
    "                assert self.cif_ids == cif_ids, print(\n",
    "                    \"order of keys is different in the json file\"\n",
    "                )\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.cif_ids)\n",
    "\n",
    "    @staticmethod\n",
    "    def make_grid_data(grid_data, emin=-5000.0, emax=5000, bins=101):\n",
    "        \"\"\"\n",
    "        make grid_data within range (emin, emax) and\n",
    "        make bins with logit function\n",
    "        and digitize (0, bins)\n",
    "        ****\n",
    "            caution : 'zero' should be padding !!\n",
    "            when you change bins, heads.MPP_heads should be changed\n",
    "        ****\n",
    "        \"\"\"\n",
    "        grid_data[grid_data <= emin] = emin\n",
    "        grid_data[grid_data > emax] = emax\n",
    "\n",
    "        x = np.linspace(emin, emax, bins)\n",
    "        new_grid_data = np.digitize(grid_data, x) + 1\n",
    "\n",
    "        return new_grid_data\n",
    "\n",
    "    @staticmethod\n",
    "    def calculate_volume(a, b, c, angle_a, angle_b, angle_c):\n",
    "        a_ = np.cos(angle_a * np.pi / 180)\n",
    "        b_ = np.cos(angle_b * np.pi / 180)\n",
    "        c_ = np.cos(angle_c * np.pi / 180)\n",
    "\n",
    "        v = a * b * c * np.sqrt(1 - a_**2 - b_**2 - c_**2 + 2 * a_ * b_ * c_)\n",
    "\n",
    "        return v.item() / (60 * 60 * 60)  # normalized volume\n",
    "\n",
    "    def get_raw_grid_data(self, cif_id):\n",
    "        file_grid = os.path.join(self.data_dir, self.split, f\"{cif_id}.grid\")\n",
    "        file_griddata = os.path.join(self.data_dir, self.split, f\"{cif_id}.griddata16\")\n",
    "\n",
    "        # get grid\n",
    "        with open(file_grid, \"r\") as f:\n",
    "            lines = f.readlines()\n",
    "            a, b, c = [float(i) for i in lines[0].split()[1:]]\n",
    "            angle_a, angle_b, angle_c = [float(i) for i in lines[1].split()[1:]]\n",
    "            cell = [int(i) for i in lines[2].split()[1:]]\n",
    "\n",
    "        volume = self.calculate_volume(a, b, c, angle_a, angle_b, angle_c)\n",
    "\n",
    "        # get grid data\n",
    "        grid_data = pickle.load(open(file_griddata, \"rb\"))\n",
    "        grid_data = self.make_grid_data(grid_data)\n",
    "        grid_data = torch.FloatTensor(grid_data)\n",
    "\n",
    "        return cell, volume, grid_data\n",
    "\n",
    "    def get_grid_data(self, cif_id, draw_false_grid=False):\n",
    "        cell, volume, grid_data = self.get_raw_grid_data(cif_id)\n",
    "        ret = {\n",
    "            \"cell\": cell,\n",
    "            \"volume\": volume,\n",
    "            \"grid_data\": grid_data,\n",
    "        }\n",
    "\n",
    "        if draw_false_grid:\n",
    "            random_index = random.randint(0, len(self.cif_ids) - 1)\n",
    "            cif_id = self.cif_ids[random_index]\n",
    "            cell, volume, grid_data = self.get_raw_grid_data(cif_id)\n",
    "            ret.update(\n",
    "                {\n",
    "                    \"false_cell\": cell,\n",
    "                    \"fale_volume\": volume,\n",
    "                    \"false_grid_data\": grid_data,\n",
    "                }\n",
    "            )\n",
    "        return ret\n",
    "\n",
    "    @staticmethod\n",
    "    def get_gaussian_distance(distances, num_step, dmax, dmin=0, var=0.2):\n",
    "        \"\"\"\n",
    "        Expands the distance by Gaussian basis\n",
    "        (https://github.com/txie-93/cgcnn.git)\n",
    "        \"\"\"\n",
    "\n",
    "        assert dmin < dmax\n",
    "        _filter = np.linspace(\n",
    "            dmin, dmax, num_step\n",
    "        )  # = np.arange(dmin, dmax + step, step) with step = 0.2\n",
    "\n",
    "        return np.exp(-((distances[..., np.newaxis] - _filter) ** 2) / var**2).float()\n",
    "\n",
    "    def get_graph(self, cif_id):\n",
    "        file_graph = os.path.join(self.data_dir, self.split, f\"{cif_id}.graphdata\")\n",
    "\n",
    "        graphdata = pickle.load(open(file_graph, \"rb\"))\n",
    "        # graphdata = [\"cif_id\", \"atom_num\", \"nbr_idx\", \"nbr_dist\", \"uni_idx\", \"uni_count\"]\n",
    "        atom_num = torch.LongTensor(graphdata[1].copy())\n",
    "        nbr_idx = torch.LongTensor(graphdata[2].copy()).view(len(atom_num), -1)\n",
    "        nbr_dist = torch.FloatTensor(graphdata[3].copy()).view(len(atom_num), -1)\n",
    "\n",
    "        nbr_fea = torch.FloatTensor(\n",
    "            self.get_gaussian_distance(nbr_dist, num_step=self.nbr_fea_len, dmax=8)\n",
    "        )\n",
    "\n",
    "        uni_idx = graphdata[4]\n",
    "        uni_count = graphdata[5]\n",
    "\n",
    "        return {\n",
    "            \"atom_num\": atom_num,\n",
    "            \"nbr_idx\": nbr_idx,\n",
    "            \"nbr_fea\": nbr_fea,\n",
    "            \"uni_idx\": uni_idx,\n",
    "            \"uni_count\": uni_count,\n",
    "        }\n",
    "\n",
    "    def get_tasks(self, index):\n",
    "        ret = dict()\n",
    "        for task, value in self.tasks.items():\n",
    "            ret.update({task: value[index]})\n",
    "\n",
    "        return ret\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        ret = dict()\n",
    "        cif_id = self.cif_ids[index]\n",
    "        target = self.targets[index]\n",
    "        add_features = self.add_features[index]  # add features list[float]\n",
    "\n",
    "        ret.update(\n",
    "            {\n",
    "                \"cif_id\": cif_id,\n",
    "                \"target\": target,\n",
    "                \"add_features\": add_features,  # add features list[float]\n",
    "            }\n",
    "        )\n",
    "        ret.update(self.get_grid_data(cif_id, draw_false_grid=self.draw_false_grid))\n",
    "        ret.update(self.get_graph(cif_id))\n",
    "\n",
    "        ret.update(self.get_tasks(index))\n",
    "\n",
    "        return ret\n",
    "\n",
    "    @staticmethod\n",
    "    def collate(batch, img_size):\n",
    "        \"\"\"\n",
    "        collate batch\n",
    "        Args:\n",
    "            batch (dict): [cif_id, atom_num, nbr_idx, nbr_fea, uni_idx, uni_count,\n",
    "                            grid_data, cell, (false_grid_data, false_cell), target]\n",
    "            img_size (int): maximum length of img size\n",
    "\n",
    "        Returns:\n",
    "            dict_batch (dict): [cif_id, atom_num, nbr_idx, nbr_fea, crystal_atom_idx,\n",
    "                                uni_idx, uni_count, grid, false_grid_data, target]\n",
    "        \"\"\"\n",
    "        batch_size = len(batch)\n",
    "        keys = set([key for b in batch for key in b.keys()])\n",
    "\n",
    "        dict_batch = {k: [dic[k] if k in dic else None for dic in batch] for k in keys}\n",
    "\n",
    "        # graph\n",
    "        batch_atom_num = dict_batch[\"atom_num\"]\n",
    "        batch_nbr_idx = dict_batch[\"nbr_idx\"]\n",
    "        batch_nbr_fea = dict_batch[\"nbr_fea\"]\n",
    "\n",
    "        crystal_atom_idx = []\n",
    "        base_idx = 0\n",
    "        for i, nbr_idx in enumerate(batch_nbr_idx):\n",
    "            n_i = nbr_idx.shape[0]\n",
    "            crystal_atom_idx.append(torch.arange(n_i) + base_idx)\n",
    "            nbr_idx += base_idx\n",
    "            base_idx += n_i\n",
    "\n",
    "        dict_batch[\"atom_num\"] = torch.cat(batch_atom_num, dim=0)\n",
    "        dict_batch[\"nbr_idx\"] = torch.cat(batch_nbr_idx, dim=0)\n",
    "        dict_batch[\"nbr_fea\"] = torch.cat(batch_nbr_fea, dim=0)\n",
    "        dict_batch[\"crystal_atom_idx\"] = crystal_atom_idx\n",
    "\n",
    "        # grid\n",
    "        batch_grid_data = dict_batch[\"grid_data\"]\n",
    "        batch_cell = dict_batch[\"cell\"]\n",
    "        new_grids = []\n",
    "\n",
    "        for bi in range(batch_size):\n",
    "            orig = batch_grid_data[bi].view(batch_cell[bi][::-1]).transpose(0, 2)\n",
    "            if batch_cell[bi] == [30, 30, 30]:  # version >= 1.1.2\n",
    "                orig = orig[None, None, :, :, :]\n",
    "            else:\n",
    "                orig = interpolate(\n",
    "                    orig[None, None, :, :, :],\n",
    "                    size=[img_size, img_size, img_size],\n",
    "                    mode=\"trilinear\",\n",
    "                    align_corners=True,\n",
    "                )\n",
    "            new_grids.append(orig)\n",
    "        new_grids = torch.concat(new_grids, axis=0)\n",
    "        dict_batch[\"grid\"] = new_grids\n",
    "\n",
    "        if \"false_grid_data\" in dict_batch.keys():\n",
    "            batch_false_grid_data = dict_batch[\"false_grid_data\"]\n",
    "            batch_false_cell = dict_batch[\"false_cell\"]\n",
    "            new_false_grids = []\n",
    "            for bi in range(batch_size):\n",
    "                orig = batch_false_grid_data[bi].view(batch_false_cell[bi])\n",
    "                if batch_cell[bi] == [30, 30, 30]:  # version >= 1.1.2\n",
    "                    orig = orig[None, None, :, :, :]\n",
    "                else:\n",
    "                    orig = interpolate(\n",
    "                        orig[None, None, :, :, :],\n",
    "                        size=[img_size, img_size, img_size],\n",
    "                        mode=\"trilinear\",\n",
    "                        align_corners=True,\n",
    "                    )\n",
    "                new_false_grids.append(orig)\n",
    "            new_false_grids = torch.concat(new_false_grids, axis=0)\n",
    "            dict_batch[\"false_grid\"] = new_false_grids\n",
    "\n",
    "        dict_batch.pop(\"grid_data\", None)\n",
    "        dict_batch.pop(\"false_grid_data\", None)\n",
    "        dict_batch.pop(\"cell\", None)\n",
    "        dict_batch.pop(\"false_cell\", None)\n",
    "\n",
    "        return dict_batch\n",
    "\n",
    "\n",
    "# MOFTransformer version 2.0.0\n",
    "import functools\n",
    "from typing import Optional\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from pytorch_lightning import LightningDataModule\n",
    "\n",
    "\n",
    "# Modified datamodule\n",
    "class Datamodule1(LightningDataModule):\n",
    "    def __init__(self, _config):\n",
    "        super().__init__()\n",
    "\n",
    "        self.data_dir = _config[\"root_dataset\"]\n",
    "\n",
    "        self.num_workers = _config[\"num_workers\"]\n",
    "        self.batch_size = _config[\"per_gpu_batchsize\"]\n",
    "        self.eval_batch_size = self.batch_size\n",
    "\n",
    "        self.draw_false_grid = _config[\"draw_false_grid\"]\n",
    "        self.img_size = _config[\"img_size\"]\n",
    "        self.downstream = _config[\"downstream\"]\n",
    "\n",
    "        self.nbr_fea_len = _config[\"nbr_fea_len\"]\n",
    "\n",
    "        self.tasks = [k for k, v in _config[\"loss_names\"].items() if v >= 1]\n",
    "\n",
    "    @property\n",
    "    def dataset_cls(self):\n",
    "        return Dataset1\n",
    "\n",
    "    def set_train_dataset(self):\n",
    "        self.train_dataset = self.dataset_cls(\n",
    "            self.data_dir,\n",
    "            split=\"train\",\n",
    "            draw_false_grid=self.draw_false_grid,\n",
    "            downstream=self.downstream,\n",
    "            nbr_fea_len=self.nbr_fea_len,\n",
    "            tasks=self.tasks,\n",
    "        )\n",
    "\n",
    "    def set_val_dataset(self):\n",
    "        self.val_dataset = self.dataset_cls(\n",
    "            self.data_dir,\n",
    "            split=\"val\",\n",
    "            draw_false_grid=self.draw_false_grid,\n",
    "            downstream=self.downstream,\n",
    "            nbr_fea_len=self.nbr_fea_len,\n",
    "            tasks=self.tasks,\n",
    "        )\n",
    "\n",
    "    def set_test_dataset(self):\n",
    "        self.test_dataset = self.dataset_cls(\n",
    "            self.data_dir,\n",
    "            split=\"test\",\n",
    "            draw_false_grid=self.draw_false_grid,\n",
    "            downstream=self.downstream,\n",
    "            nbr_fea_len=self.nbr_fea_len,\n",
    "            tasks=self.tasks,\n",
    "        )\n",
    "\n",
    "    def setup(self, stage: Optional[str] = None):\n",
    "        if stage in (None, \"fit\"):\n",
    "            self.set_train_dataset()\n",
    "            self.set_val_dataset()\n",
    "\n",
    "        if stage in (None, \"test\"):\n",
    "            self.set_test_dataset()\n",
    "\n",
    "        self.collate = functools.partial(\n",
    "            self.dataset_cls.collate,\n",
    "            img_size=self.img_size,\n",
    "        )\n",
    "\n",
    "    def train_dataloader(self) -> DataLoader:\n",
    "        return DataLoader(\n",
    "            self.train_dataset,\n",
    "            batch_size=self.batch_size,\n",
    "            num_workers=self.num_workers,\n",
    "            collate_fn=self.collate,\n",
    "            shuffle=True,\n",
    "        )\n",
    "\n",
    "    def val_dataloader(self) -> DataLoader:\n",
    "        return DataLoader(\n",
    "            self.val_dataset,\n",
    "            batch_size=self.batch_size,\n",
    "            num_workers=self.num_workers,\n",
    "            collate_fn=self.collate,\n",
    "        )\n",
    "\n",
    "    def test_dataloader(self) -> DataLoader:\n",
    "        return DataLoader(\n",
    "            self.test_dataset,\n",
    "            batch_size=self.batch_size,\n",
    "            num_workers=self.num_workers,\n",
    "            collate_fn=self.collate,\n",
    "        )\n",
    "\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchmetrics.functional import mean_absolute_error\n",
    "\n",
    "\n",
    "def init_weights(module):\n",
    "    if isinstance(module, (nn.Linear, nn.Embedding)):\n",
    "        module.weight.data.normal_(mean=0.0, std=0.02)\n",
    "    elif isinstance(module, nn.LayerNorm):\n",
    "        module.bias.data.zero_()\n",
    "        module.weight.data.fill_(1.0)\n",
    "\n",
    "    if isinstance(module, nn.Linear) and module.bias is not None:\n",
    "        module.bias.data.zero_()\n",
    "\n",
    "\n",
    "# modified loss function with uncertainty logarithm\n",
    "def compute_loss(logits, labels, log_sigma):\n",
    "    loss = 0.5 * torch.exp(-log_sigma) * (labels - logits) ** 2 + 0.5 * log_sigma\n",
    "    return loss.mean()\n",
    "\n",
    "\n",
    "def compute_regression(pl_module, batch, normalizer):\n",
    "    infer = pl_module.infer(batch)\n",
    "\n",
    "    out = pl_module.regression_head(infer[\"cls_feats\"])\n",
    "    logits = out[:, 0]  # [B]\n",
    "    log_sigma = out[:, 1]  # uncertainty logarithm\n",
    "    labels = torch.FloatTensor(batch[\"target\"]).to(logits.device)  # [B]\n",
    "    assert len(labels.shape) == 1\n",
    "\n",
    "    # normalize encode if config[\"mean\"] and config[\"std], else pass\n",
    "    labels = normalizer.encode(labels)\n",
    "    loss = compute_loss(logits, labels, log_sigma)\n",
    "\n",
    "    labels = labels.to(torch.float32)\n",
    "    logits = logits.to(torch.float32)\n",
    "\n",
    "    ret = {\n",
    "        \"cif_id\": infer[\"cif_id\"],\n",
    "        \"cls_feats\": infer[\"cls_feats\"],\n",
    "        \"regression_loss\": loss,\n",
    "        \"regression_logits\": normalizer.decode(logits),\n",
    "        \"regression_labels\": normalizer.decode(labels),\n",
    "        \"log_sigma\": log_sigma,  # uncertainty logarithm\n",
    "    }\n",
    "\n",
    "    # call update() loss and acc\n",
    "    phase = \"train\" if pl_module.training else \"val\"\n",
    "    loss = getattr(pl_module, f\"{phase}_regression_loss\")(ret[\"regression_loss\"])\n",
    "    mae = getattr(pl_module, f\"{phase}_regression_mae\")(\n",
    "        mean_absolute_error(\n",
    "            ret[\"regression_logits\"].cpu(), ret[\"regression_labels\"].cpu()\n",
    "        )\n",
    "    )\n",
    "\n",
    "    if pl_module.write_log:\n",
    "        pl_module.log(f\"regression/{phase}/loss\", loss, sync_dist=True)\n",
    "        pl_module.log(f\"regression/{phase}/mae\", mae, sync_dist=True)\n",
    "\n",
    "    return ret\n",
    "\n",
    "\n",
    "def compute_classification(pl_module, batch):\n",
    "    infer = pl_module.infer(batch)\n",
    "\n",
    "    logits, binary = pl_module.classification_head(\n",
    "        infer[\"cls_feats\"]\n",
    "    )  # [B, output_dim]\n",
    "    labels = torch.LongTensor(batch[\"target\"]).to(logits.device)  # [B]\n",
    "    assert len(labels.shape) == 1\n",
    "    if binary:\n",
    "        logits = logits.squeeze(dim=-1)\n",
    "        loss = F.binary_cross_entropy_with_logits(input=logits, target=labels.float())\n",
    "    else:\n",
    "        loss = F.cross_entropy(logits, labels)\n",
    "\n",
    "    ret = {\n",
    "        \"cif_id\": infer[\"cif_id\"],\n",
    "        \"cls_feats\": infer[\"cls_feats\"],\n",
    "        \"classification_loss\": loss,\n",
    "        \"classification_logits\": logits,\n",
    "        \"classification_labels\": labels,\n",
    "    }\n",
    "\n",
    "    # call update() loss and acc\n",
    "    phase = \"train\" if pl_module.training else \"val\"\n",
    "    loss = getattr(pl_module, f\"{phase}_classification_loss\")(\n",
    "        ret[\"classification_loss\"]\n",
    "    )\n",
    "    acc = getattr(pl_module, f\"{phase}_classification_accuracy\")(\n",
    "        torch.sigmoid(ret[\"classification_logits\"]),\n",
    "        ret[\"classification_labels\"],  # у авторов не было сигмоиды, но она нужна\n",
    "    )\n",
    "\n",
    "    if pl_module.write_log:\n",
    "        pl_module.log(f\"classification/{phase}/loss\", loss, sync_dist=True)\n",
    "        pl_module.log(f\"classification/{phase}/accuracy\", acc, sync_dist=True)\n",
    "\n",
    "    return ret\n",
    "\n",
    "\n",
    "def compute_mpp(pl_module, batch):\n",
    "    infer = pl_module.infer(batch, mask_grid=True)\n",
    "\n",
    "    mpp_logits = pl_module.mpp_head(infer[\"grid_feats\"])  # [B, max_image_len+2, bins]\n",
    "    mpp_logits = mpp_logits[\n",
    "        :, :-1, :\n",
    "    ]  # ignore volume embedding, [B, max_image_len+1, bins]\n",
    "    mpp_labels = infer[\"grid_labels\"]  # [B, max_image_len+1, C=1]\n",
    "\n",
    "    mask = mpp_labels != -100.0  # [B, max_image_len, 1]\n",
    "\n",
    "    # masking\n",
    "    mpp_logits = mpp_logits[mask.squeeze(-1)]  # [mask, bins]\n",
    "    mpp_labels = mpp_labels[mask].long()  # [mask]\n",
    "\n",
    "    mpp_loss = F.cross_entropy(mpp_logits, mpp_labels)\n",
    "\n",
    "    ret = {\n",
    "        \"mpp_loss\": mpp_loss,\n",
    "        \"mpp_logits\": mpp_logits,\n",
    "        \"mpp_labels\": mpp_labels,\n",
    "    }\n",
    "\n",
    "    # call update() loss and acc\n",
    "    phase = \"train\" if pl_module.training else \"val\"\n",
    "    loss = getattr(pl_module, f\"{phase}_mpp_loss\")(ret[\"mpp_loss\"])\n",
    "    acc = getattr(pl_module, f\"{phase}_mpp_accuracy\")(\n",
    "        ret[\"mpp_logits\"], ret[\"mpp_labels\"]\n",
    "    )\n",
    "\n",
    "    if pl_module.write_log:\n",
    "        pl_module.log(f\"mpp/{phase}/loss\", loss, sync_dist=True)\n",
    "        pl_module.log(f\"mpp/{phase}/accuracy\", acc, sync_dist=True)\n",
    "\n",
    "    return ret\n",
    "\n",
    "\n",
    "def compute_mtp(pl_module, batch):\n",
    "    infer = pl_module.infer(batch)\n",
    "    mtp_logits = pl_module.mtp_head(infer[\"cls_feats\"])  # [B, hid_dim]\n",
    "    mtp_labels = torch.LongTensor(batch[\"mtp\"]).to(mtp_logits.device)  # [B]\n",
    "\n",
    "    mtp_loss = F.cross_entropy(mtp_logits, mtp_labels)  # [B]\n",
    "\n",
    "    ret = {\n",
    "        \"mtp_loss\": mtp_loss,\n",
    "        \"mtp_logits\": mtp_logits,\n",
    "        \"mtp_labels\": mtp_labels,\n",
    "    }\n",
    "\n",
    "    # call update() loss and acc\n",
    "    phase = \"train\" if pl_module.training else \"val\"\n",
    "    loss = getattr(pl_module, f\"{phase}_mtp_loss\")(ret[\"mtp_loss\"])\n",
    "    acc = getattr(pl_module, f\"{phase}_mtp_accuracy\")(\n",
    "        ret[\"mtp_logits\"], ret[\"mtp_labels\"]\n",
    "    )\n",
    "\n",
    "    if pl_module.write_log:\n",
    "        pl_module.log(f\"mtp/{phase}/loss\", loss, sync_dist=True)\n",
    "        pl_module.log(f\"mtp/{phase}/accuracy\", acc, sync_dist=True)\n",
    "\n",
    "    return ret\n",
    "\n",
    "\n",
    "def compute_vfp(pl_module, batch):\n",
    "    infer = pl_module.infer(batch)\n",
    "\n",
    "    vfp_logits = pl_module.vfp_head(infer[\"cls_feats\"]).squeeze(-1)  # [B]\n",
    "    vfp_labels = torch.FloatTensor(batch[\"vfp\"]).to(vfp_logits.device)\n",
    "\n",
    "    assert len(vfp_labels.shape) == 1\n",
    "\n",
    "    vfp_loss = F.mse_loss(vfp_logits, vfp_labels)\n",
    "    ret = {\n",
    "        \"vfp_loss\": vfp_loss,\n",
    "        \"vfp_logits\": vfp_logits,\n",
    "        \"vfp_labels\": vfp_labels,\n",
    "    }\n",
    "\n",
    "    # call update() loss and acc\n",
    "    phase = \"train\" if pl_module.training else \"val\"\n",
    "    loss = getattr(pl_module, f\"{phase}_vfp_loss\")(ret[\"vfp_loss\"])\n",
    "    mae = getattr(pl_module, f\"{phase}_vfp_mae\")(\n",
    "        mean_absolute_error(ret[\"vfp_logits\"], ret[\"vfp_labels\"])\n",
    "    )\n",
    "\n",
    "    if pl_module.write_log:\n",
    "        pl_module.log(f\"vfp/{phase}/loss\", loss, sync_dist=True)\n",
    "        pl_module.log(f\"vfp/{phase}/mae\", mae, sync_dist=True)\n",
    "\n",
    "    return ret\n",
    "\n",
    "\n",
    "def compute_ggm(pl_module, batch):\n",
    "    pos_len = len(batch[\"grid\"]) // 2\n",
    "    neg_len = len(batch[\"grid\"]) - pos_len\n",
    "    ggm_labels = torch.cat([torch.ones(pos_len), torch.zeros(neg_len)]).to(\n",
    "        pl_module.device\n",
    "    )\n",
    "\n",
    "    ggm_images = []\n",
    "    for i, (bti, bfi) in enumerate(zip(batch[\"grid\"], batch[\"false_grid\"])):\n",
    "        if ggm_labels[i] == 1:\n",
    "            ggm_images.append(bti)\n",
    "        else:\n",
    "            ggm_images.append(bfi)\n",
    "\n",
    "    ggm_images = torch.stack(ggm_images, dim=0)\n",
    "\n",
    "    batch = {k: v for k, v in batch.items()}\n",
    "    batch[\"grid\"] = ggm_images\n",
    "\n",
    "    infer = pl_module.infer(batch)\n",
    "    ggm_logits = pl_module.ggm_head(infer[\"cls_feats\"])  # cls_feats\n",
    "    ggm_loss = F.cross_entropy(ggm_logits, ggm_labels.long())\n",
    "\n",
    "    ret = {\n",
    "        \"ggm_loss\": ggm_loss,\n",
    "        \"ggm_logits\": ggm_logits,\n",
    "        \"ggm_labels\": ggm_labels,\n",
    "    }\n",
    "\n",
    "    # call update() loss and acc\n",
    "    phase = \"train\" if pl_module.training else \"val\"\n",
    "    loss = getattr(pl_module, f\"{phase}_ggm_loss\")(ret[\"ggm_loss\"])\n",
    "    acc = getattr(pl_module, f\"{phase}_ggm_accuracy\")(\n",
    "        ret[\"ggm_logits\"], ret[\"ggm_labels\"]\n",
    "    )\n",
    "\n",
    "    if pl_module.write_log:\n",
    "        pl_module.log(f\"ggm/{phase}/loss\", loss, sync_dist=True)\n",
    "        pl_module.log(f\"ggm/{phase}/accuracy\", acc, sync_dist=True)\n",
    "\n",
    "    return ret\n",
    "\n",
    "\n",
    "def compute_moc(pl_module, batch):\n",
    "    if \"bbc\" in batch.keys():\n",
    "        task = \"bbc\"\n",
    "    else:\n",
    "        task = \"moc\"\n",
    "\n",
    "    infer = pl_module.infer(batch)\n",
    "    moc_logits = pl_module.moc_head(\n",
    "        infer[\"graph_feats\"][:, 1:, :]\n",
    "    ).flatten()  # [B, max_graph_len] -> [B * max_graph_len]\n",
    "    moc_labels = (\n",
    "        infer[\"mo_labels\"].to(moc_logits).flatten()\n",
    "    )  # [B, max_graph_len] -> [B * max_graph_len]\n",
    "    mask = moc_labels != -100\n",
    "\n",
    "    moc_loss = F.binary_cross_entropy_with_logits(\n",
    "        input=moc_logits[mask], target=moc_labels[mask]\n",
    "    )  # [B * max_graph_len]\n",
    "\n",
    "    ret = {\n",
    "        \"moc_loss\": moc_loss,\n",
    "        \"moc_logits\": moc_logits,\n",
    "        \"moc_labels\": moc_labels,\n",
    "    }\n",
    "\n",
    "    # call update() loss and acc\n",
    "    phase = \"train\" if pl_module.training else \"val\"\n",
    "    loss = getattr(pl_module, f\"{phase}_{task}_loss\")(ret[\"moc_loss\"])\n",
    "    acc = getattr(pl_module, f\"{phase}_{task}_accuracy\")(\n",
    "        nn.Sigmoid()(ret[\"moc_logits\"]), ret[\"moc_labels\"].long()\n",
    "    )\n",
    "\n",
    "    if pl_module.write_log:\n",
    "        pl_module.log(f\"{task}/{phase}/loss\", loss, sync_dist=True)\n",
    "        pl_module.log(f\"{task}/{phase}/accuracy\", acc, sync_dist=True)\n",
    "\n",
    "    return ret\n",
    "\n",
    "\n",
    "# Regression head\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class RegressionHead(nn.Module):\n",
    "    \"\"\"\n",
    "    Modified head for Regression\n",
    "    Original: self.layer = nn.Linear(hid_dim, 1)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, hid_dim):\n",
    "        super().__init__()\n",
    "        self.layer = nn.Sequential(\n",
    "            nn.Linear(hid_dim, 2048), nn.ReLU(), nn.Linear(2048, 2)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.layer(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "# Classification head (original MOFTransformer)\n",
    "class ClassificationHead(nn.Module):\n",
    "    \"\"\"\n",
    "    head for Classification\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, hid_dim, n_classes):\n",
    "        super().__init__()\n",
    "\n",
    "        if n_classes == 2:\n",
    "            self.layer = nn.Sequential(\n",
    "                nn.Linear(hid_dim, 1),\n",
    "            )\n",
    "            self.binary = True\n",
    "        else:\n",
    "            self.layer = nn.Sequential(nn.Linear(hid_dim, 1))\n",
    "            self.binary = False\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.layer(x)\n",
    "\n",
    "        return x, self.binary\n",
    "\n",
    "\n",
    "# MOFTransformer version 2.1.0\n",
    "from typing import Any, List\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning import LightningModule\n",
    "\n",
    "from moftransformer.modules import objectives, heads, module_utils\n",
    "from moftransformer.modules.cgcnn import GraphEmbeddings\n",
    "from moftransformer.modules.vision_transformer_3d import VisionTransformer3D\n",
    "\n",
    "from moftransformer.modules.module_utils import Normalizer\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "\n",
    "# Modified model\n",
    "class Module1(LightningModule):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "\n",
    "        self.max_grid_len = config[\"max_grid_len\"]\n",
    "        self.vis = config[\"visualize\"]\n",
    "\n",
    "        # graph embedding with_unique_atoms\n",
    "        self.graph_embeddings = GraphEmbeddings(\n",
    "            atom_fea_len=config[\"atom_fea_len\"],\n",
    "            nbr_fea_len=config[\"nbr_fea_len\"],\n",
    "            max_graph_len=config[\"max_graph_len\"],\n",
    "            hid_dim=config[\"hid_dim\"],\n",
    "            vis=config[\"visualize\"],\n",
    "            n_conv=3,\n",
    "        )\n",
    "        self.graph_embeddings.apply(objectives.init_weights)\n",
    "\n",
    "        # token type embeddings\n",
    "        self.token_type_embeddings = nn.Embedding(2, config[\"hid_dim\"])\n",
    "        self.token_type_embeddings.apply(objectives.init_weights)\n",
    "\n",
    "        # set transformer\n",
    "        self.transformer = VisionTransformer3D(\n",
    "            img_size=config[\"img_size\"],\n",
    "            patch_size=config[\"patch_size\"],\n",
    "            in_chans=config[\"in_chans\"],\n",
    "            embed_dim=config[\"hid_dim\"],\n",
    "            depth=config[\"num_layers\"],\n",
    "            num_heads=config[\"num_heads\"],\n",
    "            mlp_ratio=config[\"mlp_ratio\"],\n",
    "            drop_rate=config[\"drop_rate\"],\n",
    "            mpp_ratio=config[\"mpp_ratio\"],\n",
    "        )\n",
    "\n",
    "        # class token\n",
    "        self.cls_embeddings = nn.Linear(1, config[\"hid_dim\"])\n",
    "        self.cls_embeddings.apply(objectives.init_weights)\n",
    "\n",
    "        # volume token\n",
    "        self.volume_embeddings = nn.Linear(1, config[\"hid_dim\"])\n",
    "        self.volume_embeddings.apply(objectives.init_weights)\n",
    "\n",
    "        # add features tokens\n",
    "        self.add_features_embeddings = nn.Linear(\n",
    "            N_ADD_FEATURES, config[\"hid_dim\"]\n",
    "        )  # linear embedding for add features\n",
    "        self.add_features_embeddings.apply(objectives.init_weights)\n",
    "\n",
    "        self.pooler = heads.Pooler(config[\"hid_dim\"])\n",
    "        self.pooler.apply(objectives.init_weights)\n",
    "\n",
    "        # ===================== loss =====================\n",
    "        if config[\"loss_names\"][\"ggm\"] > 0:\n",
    "            self.ggm_head = heads.GGMHead(config[\"hid_dim\"])\n",
    "            self.ggm_head.apply(objectives.init_weights)\n",
    "\n",
    "        if config[\"loss_names\"][\"mpp\"] > 0:\n",
    "            self.mpp_head = heads.MPPHead(config[\"hid_dim\"])\n",
    "            self.mpp_head.apply(objectives.init_weights)\n",
    "\n",
    "        if config[\"loss_names\"][\"mtp\"] > 0:\n",
    "            self.mtp_head = heads.MTPHead(config[\"hid_dim\"])\n",
    "            self.mtp_head.apply(objectives.init_weights)\n",
    "\n",
    "        if config[\"loss_names\"][\"vfp\"] > 0:\n",
    "            self.vfp_head = heads.VFPHead(config[\"hid_dim\"])\n",
    "            self.vfp_head.apply(objectives.init_weights)\n",
    "\n",
    "        if config[\"loss_names\"][\"moc\"] > 0 or config[\"loss_names\"][\"bbc\"] > 0:\n",
    "            self.moc_head = heads.MOCHead(config[\"hid_dim\"])\n",
    "            self.moc_head.apply(objectives.init_weights)\n",
    "\n",
    "        # ===================== Downstream =====================\n",
    "        hid_dim = config[\"hid_dim\"]\n",
    "\n",
    "        if config[\"load_path\"] != \"\" and not config[\"test_only\"]:\n",
    "            ckpt = torch.load(self.hparams.config[\"load_path\"], map_location=\"cpu\")\n",
    "            state_dict = ckpt[\"state_dict\"]\n",
    "            self.load_state_dict(state_dict, strict=False)\n",
    "            print(f\"load model : {config['load_path']}\")\n",
    "\n",
    "        if self.hparams.config[\"loss_names\"][\"regression\"] > 0:\n",
    "            self.regression_head = RegressionHead(hid_dim)\n",
    "            self.regression_head.apply(objectives.init_weights)\n",
    "            # normalization\n",
    "            self.mean = config[\"mean\"]\n",
    "            self.std = config[\"std\"]\n",
    "\n",
    "        if self.hparams.config[\"loss_names\"][\"classification\"] > 0:\n",
    "            n_classes = config[\"n_classes\"]\n",
    "            self.classification_head = heads.ClassificationHead(hid_dim, n_classes)\n",
    "            self.classification_head.apply(objectives.init_weights)\n",
    "\n",
    "        module_utils.set_metrics(self)\n",
    "        self.current_tasks = list()\n",
    "        # ===================== load downstream (test_only) ======================\n",
    "\n",
    "        if config[\"load_path\"] != \"\" and config[\"test_only\"]:\n",
    "            ckpt = torch.load(config[\"load_path\"], map_location=\"cpu\")\n",
    "            state_dict = ckpt[\"state_dict\"]\n",
    "            self.load_state_dict(state_dict, strict=False)\n",
    "            print(f\"load model : {config['load_path']}\")\n",
    "\n",
    "        self.test_logits = []\n",
    "        self.test_labels = []\n",
    "        self.test_cifid = []\n",
    "        self.write_log = True\n",
    "\n",
    "    def infer(\n",
    "        self,\n",
    "        batch,\n",
    "        mask_grid=False,\n",
    "    ):\n",
    "        cif_id = batch[\"cif_id\"]\n",
    "        atom_num = batch[\"atom_num\"]  # [N']\n",
    "        nbr_idx = batch[\"nbr_idx\"]  # [N', M]\n",
    "        nbr_fea = batch[\"nbr_fea\"]  # [N', M, nbr_fea_len]\n",
    "        crystal_atom_idx = batch[\"crystal_atom_idx\"]  # list [B]\n",
    "        uni_idx = batch[\"uni_idx\"]  # list [B]\n",
    "        uni_count = batch[\"uni_count\"]  # list [B]\n",
    "\n",
    "        grid = batch[\"grid\"]  # [B, C, H, W, D]\n",
    "        volume = batch[\"volume\"]  # list [B]\n",
    "        add_features = batch[\"add_features\"]  # add features [B, N_ADD_FEATURES]\n",
    "\n",
    "        if \"moc\" in batch.keys():\n",
    "            moc = batch[\"moc\"]  # [B]\n",
    "        elif \"bbc\" in batch.keys():\n",
    "            moc = batch[\"bbc\"]  # [B]\n",
    "        else:\n",
    "            moc = None\n",
    "\n",
    "        # get graph embeds\n",
    "        (\n",
    "            graph_embeds,  # [B, max_graph_len, hid_dim],\n",
    "            graph_masks,  # [B, max_graph_len],\n",
    "            mo_labels,  # if moc: [B, max_graph_len], else: None\n",
    "        ) = self.graph_embeddings(\n",
    "            atom_num=atom_num,\n",
    "            nbr_idx=nbr_idx,\n",
    "            nbr_fea=nbr_fea,\n",
    "            crystal_atom_idx=crystal_atom_idx,\n",
    "            uni_idx=uni_idx,\n",
    "            uni_count=uni_count,\n",
    "            moc=moc,\n",
    "        )\n",
    "        # add class embeds to graph_embeds\n",
    "        cls_tokens = torch.zeros(len(crystal_atom_idx)).to(graph_embeds)  # [B]\n",
    "        cls_embeds = self.cls_embeddings(cls_tokens[:, None, None])  # [B, 1, hid_dim]\n",
    "        cls_mask = torch.ones(len(crystal_atom_idx), 1).to(graph_masks)  # [B, 1]\n",
    "\n",
    "        graph_embeds = torch.cat(\n",
    "            [cls_embeds, graph_embeds], dim=1\n",
    "        )  # [B, max_graph_len+1, hid_dim]\n",
    "        graph_masks = torch.cat([cls_mask, graph_masks], dim=1)  # [B, max_graph_len+1]\n",
    "\n",
    "        # get grid embeds\n",
    "        (\n",
    "            grid_embeds,  # [B, max_grid_len+1, hid_dim]\n",
    "            grid_masks,  # [B, max_grid_len+1]\n",
    "            grid_labels,  # [B, grid+1, C] if mask_image == True\n",
    "        ) = self.transformer.visual_embed(\n",
    "            grid,\n",
    "            max_image_len=self.max_grid_len,\n",
    "            mask_it=mask_grid,\n",
    "        )\n",
    "\n",
    "        # add volume embeds to grid_embeds\n",
    "        volume = torch.FloatTensor(volume).to(grid_embeds)  # [B]\n",
    "        volume_embeds = self.volume_embeddings(volume[:, None, None])  # [B, 1, hid_dim]\n",
    "        volume_mask = torch.ones(volume.shape[0], 1).to(grid_masks)\n",
    "        # add add_features embeds to grid_embeds\n",
    "        add_features = torch.FloatTensor(add_features).to(grid_embeds)  # [B]\n",
    "        add_features_embeds = self.add_features_embeddings(\n",
    "            add_features[:, None]\n",
    "        )  # [B, 1, hid_dim]\n",
    "        add_features_mask = torch.ones(add_features.shape[0], 1).to(grid_masks)\n",
    "        grid_embeds = torch.cat(\n",
    "            [grid_embeds, volume_embeds, add_features_embeds], dim=1\n",
    "        )  # [B, max_grid_len+2, hid_dim]\n",
    "        grid_masks = torch.cat(\n",
    "            [grid_masks, volume_mask, add_features_mask], dim=1\n",
    "        )  # [B, max_grid_len+2]\n",
    "\n",
    "        # add token_type_embeddings\n",
    "        graph_embeds = graph_embeds + self.token_type_embeddings(\n",
    "            torch.zeros_like(graph_masks, device=self.device).long()\n",
    "        )\n",
    "        grid_embeds = grid_embeds + self.token_type_embeddings(\n",
    "            torch.ones_like(grid_masks, device=self.device).long()\n",
    "        )\n",
    "\n",
    "        co_embeds = torch.cat(\n",
    "            [graph_embeds, grid_embeds], dim=1\n",
    "        )  # [B, final_max_len, hid_dim]\n",
    "        co_masks = torch.cat(\n",
    "            [graph_masks, grid_masks], dim=1\n",
    "        )  # [B, final_max_len, hid_dim]\n",
    "\n",
    "        x = co_embeds\n",
    "\n",
    "        attn_weights = []\n",
    "        for i, blk in enumerate(self.transformer.blocks):\n",
    "            x, _attn = blk(x, mask=co_masks)\n",
    "\n",
    "            if self.vis:\n",
    "                attn_weights.append(_attn)\n",
    "\n",
    "        x = self.transformer.norm(x)\n",
    "        graph_feats, grid_feats = (\n",
    "            x[:, : graph_embeds.shape[1]],\n",
    "            x[:, graph_embeds.shape[1] :],\n",
    "        )  # [B, max_graph_len, hid_dim], [B, max_grid_len+2, hid_dim]\n",
    "\n",
    "        cls_feats = self.pooler(x)  # [B, hid_dim]\n",
    "\n",
    "        ret = {\n",
    "            \"graph_feats\": graph_feats,\n",
    "            \"grid_feats\": grid_feats,\n",
    "            \"cls_feats\": cls_feats,\n",
    "            \"raw_cls_feats\": x[:, 0],\n",
    "            \"graph_masks\": graph_masks,\n",
    "            \"grid_masks\": grid_masks,\n",
    "            \"grid_labels\": grid_labels,  # if MPP, else None\n",
    "            \"mo_labels\": mo_labels,  # if MOC, else None\n",
    "            \"cif_id\": cif_id,\n",
    "            \"attn_weights\": attn_weights,\n",
    "            \"add_features\": torch.tensor(batch[\"add_features\"]).to(\n",
    "                cls_feats\n",
    "            ),  # add features\n",
    "        }\n",
    "\n",
    "        return ret\n",
    "\n",
    "    def forward(self, batch):\n",
    "        ret = dict()\n",
    "\n",
    "        if len(self.current_tasks) == 0:\n",
    "            ret.update(self.infer(batch))\n",
    "            return ret\n",
    "\n",
    "        # Masked Patch Prediction\n",
    "        if \"mpp\" in self.current_tasks:\n",
    "            ret.update(objectives.compute_mpp(self, batch))\n",
    "\n",
    "        # Graph Grid Matching\n",
    "        if \"ggm\" in self.current_tasks:\n",
    "            ret.update(objectives.compute_ggm(self, batch))\n",
    "\n",
    "        # MOF Topology Prediction\n",
    "        if \"mtp\" in self.current_tasks:\n",
    "            ret.update(objectives.compute_mtp(self, batch))\n",
    "\n",
    "        # Void Fraction Prediction\n",
    "        if \"vfp\" in self.current_tasks:\n",
    "            ret.update(objectives.compute_vfp(self, batch))\n",
    "\n",
    "        # Metal Organic Classification (or Building Block Classfication)\n",
    "        if \"moc\" in self.current_tasks or \"bbc\" in self.current_tasks:\n",
    "            ret.update(objectives.compute_moc(self, batch))\n",
    "\n",
    "        # regression\n",
    "        if \"regression\" in self.current_tasks:\n",
    "            normalizer = Normalizer(self.mean, self.std)\n",
    "            # ret.update(objectives.compute_regression(self, batch, normalizer))\n",
    "            ret.update(compute_regression(self, batch, normalizer))\n",
    "\n",
    "        # classification\n",
    "        if \"classification\" in self.current_tasks:\n",
    "            # ret.update(objectives.compute_classification(self, batch))\n",
    "            ret.update(compute_classification(self, batch))\n",
    "        return ret\n",
    "\n",
    "    def on_train_start(self):\n",
    "        module_utils.set_task(self)\n",
    "        self.write_log = True\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        output = self(batch)\n",
    "        total_loss = sum([v for k, v in output.items() if \"loss\" in k])\n",
    "        return total_loss\n",
    "\n",
    "    def on_train_epoch_end(self):\n",
    "        module_utils.epoch_wrapup(self)\n",
    "\n",
    "    def on_validation_start(self):\n",
    "        module_utils.set_task(self)\n",
    "        self.write_log = True\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        output = self(batch)\n",
    "\n",
    "    def on_validation_epoch_end(self) -> None:\n",
    "        module_utils.epoch_wrapup(self)\n",
    "\n",
    "    def on_test_start(\n",
    "        self,\n",
    "    ):\n",
    "        module_utils.set_task(self)\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        output = self(batch)\n",
    "        output = {\n",
    "            k: (v.cpu() if torch.is_tensor(v) else v) for k, v in output.items()\n",
    "        }  # update cpu for memory\n",
    "\n",
    "        if \"regression_logits\" in output.keys():\n",
    "            self.test_logits += output[\"regression_logits\"].tolist()\n",
    "            self.test_labels += output[\"regression_labels\"].tolist()\n",
    "        return output\n",
    "\n",
    "    def on_test_epoch_end(self):\n",
    "        module_utils.epoch_wrapup(self)\n",
    "\n",
    "        # calculate r2 score when regression\n",
    "        if len(self.test_logits) > 1:\n",
    "            r2 = r2_score(np.array(self.test_labels), np.array(self.test_logits))\n",
    "            self.log(f\"test/r2_score\", r2, sync_dist=True)\n",
    "            self.test_labels.clear()\n",
    "            self.test_logits.clear()\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return module_utils.set_schedule(self)\n",
    "\n",
    "    def on_predict_start(self):\n",
    "        self.write_log = False\n",
    "        module_utils.set_task(self)\n",
    "\n",
    "    def predict_step(self, batch, batch_idx, dataloader_idx=0):\n",
    "        output = self(batch)\n",
    "\n",
    "        if \"classification_logits\" in output:\n",
    "            if self.hparams.config[\"n_classes\"] == 2:\n",
    "                output[\"classification_logits_index\"] = torch.round(\n",
    "                    torch.sigmoid(output[\"classification_logits\"])\n",
    "                ).to(\n",
    "                    torch.int\n",
    "                )  # added sigmoid\n",
    "            else:\n",
    "                softmax = torch.nn.Softmax(dim=1)\n",
    "                output[\"classification_logits\"] = softmax(\n",
    "                    output[\"classification_logits\"]\n",
    "                )\n",
    "                output[\"classification_logits_index\"] = torch.argmax(\n",
    "                    output[\"classification_logits\"], dim=1\n",
    "                )\n",
    "\n",
    "        output = {\n",
    "            k: (v.cpu().tolist() if torch.is_tensor(v) else v)\n",
    "            for k, v in output.items()\n",
    "            if (\"logits\" in k)\n",
    "            or (\"labels\" in k)\n",
    "            or (\"sigma\" in k)\n",
    "            or \"cif_id\" == k  # added output of sigma\n",
    "        }\n",
    "\n",
    "        return output\n",
    "\n",
    "    def on_predict_epoch_end(self, *args):\n",
    "        self.test_labels.clear()\n",
    "        self.test_logits.clear()\n",
    "\n",
    "    def on_predict_end(\n",
    "        self,\n",
    "    ):\n",
    "        self.write_log = True\n",
    "\n",
    "    def lr_scheduler_step(self, scheduler, *args):\n",
    "        if len(args) == 2:\n",
    "            optimizer_idx, metric = args\n",
    "        elif len(args) == 1:\n",
    "            (metric,) = args\n",
    "        else:\n",
    "            raise ValueError(\n",
    "                \"lr_scheduler_step must have metric and optimizer_idx(optional)\"\n",
    "            )\n",
    "\n",
    "        if pl.__version__ >= \"2.0.0\":\n",
    "            scheduler.step(epoch=self.current_epoch)\n",
    "        else:\n",
    "            scheduler.step()\n",
    "\n",
    "\n",
    "import sys\n",
    "import os\n",
    "import copy\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "import shutil\n",
    "\n",
    "import pytorch_lightning as pl\n",
    "\n",
    "from moftransformer.config import ex\n",
    "from moftransformer.config import config as _config\n",
    "from moftransformer.utils.validation import (\n",
    "    get_valid_config,\n",
    "    get_num_devices,\n",
    "    ConfigurationError,\n",
    ")\n",
    "\n",
    "warnings.filterwarnings(\n",
    "    \"ignore\", \".*Trying to infer the `batch_size` from an ambiguous collection.*\"\n",
    ")\n",
    "\n",
    "\n",
    "_IS_INTERACTIVE = hasattr(sys, \"ps1\")\n",
    "\n",
    "\n",
    "def run(root_dataset, downstream=None, log_dir=\"logs/\", *, test_only=False, **kwargs):\n",
    "\n",
    "    config = copy.deepcopy(_config())\n",
    "    for key in kwargs.keys():\n",
    "        if key not in config:\n",
    "            raise ConfigurationError(f\"{key} is not in configuration.\")\n",
    "\n",
    "    config.update(kwargs)\n",
    "    config[\"root_dataset\"] = root_dataset\n",
    "    config[\"downstream\"] = downstream\n",
    "    config[\"log_dir\"] = log_dir\n",
    "    config[\"test_only\"] = test_only\n",
    "\n",
    "    main1(config)\n",
    "\n",
    "\n",
    "# @ex.automain\n",
    "def main1(_config):\n",
    "    _config = copy.deepcopy(_config)\n",
    "    pl.seed_everything(_config[\"seed\"])\n",
    "\n",
    "    _config = get_valid_config(_config)\n",
    "    dm = Datamodule1(_config)\n",
    "    model = Module1(_config)\n",
    "\n",
    "    exp_name = f\"{_config['exp_name']}\"\n",
    "\n",
    "    os.makedirs(_config[\"log_dir\"], exist_ok=True)\n",
    "    checkpoint_callback = pl.callbacks.ModelCheckpoint(\n",
    "        save_top_k=1,\n",
    "        verbose=True,\n",
    "        monitor=\"val/the_metric\",\n",
    "        mode=\"max\",\n",
    "        save_last=True,\n",
    "    )\n",
    "\n",
    "    if _config[\"test_only\"]:\n",
    "        name = f'test_{exp_name}_seed{_config[\"seed\"]}_from_{str(_config[\"load_path\"]).split(\"/\")[-1][:-5]}'\n",
    "    else:\n",
    "        name = f'{exp_name}_seed{_config[\"seed\"]}_from_{str(_config[\"load_path\"]).split(\"/\")[-1][:-5]}'\n",
    "\n",
    "    logger = pl.loggers.TensorBoardLogger(\n",
    "        _config[\"log_dir\"],\n",
    "        name=name,\n",
    "    )\n",
    "\n",
    "    lr_callback = pl.callbacks.LearningRateMonitor(logging_interval=\"step\")\n",
    "    callbacks = [checkpoint_callback, lr_callback]\n",
    "\n",
    "    num_device = get_num_devices(_config)\n",
    "    print(\"num_device\", num_device)\n",
    "\n",
    "    # gradient accumulation\n",
    "    if num_device == 0:\n",
    "        accumulate_grad_batches = _config[\"batch_size\"] // (\n",
    "            _config[\"per_gpu_batchsize\"] * _config[\"num_nodes\"]\n",
    "        )\n",
    "    else:\n",
    "        accumulate_grad_batches = _config[\"batch_size\"] // (\n",
    "            _config[\"per_gpu_batchsize\"] * num_device * _config[\"num_nodes\"]\n",
    "        )\n",
    "\n",
    "    max_steps = _config[\"max_steps\"] if _config[\"max_steps\"] is not None else None\n",
    "\n",
    "    if _IS_INTERACTIVE:\n",
    "        strategy = None\n",
    "    elif pl.__version__ >= \"2.0.0\":\n",
    "        strategy = \"ddp_find_unused_parameters_true\"\n",
    "    else:\n",
    "        strategy = \"ddp\"\n",
    "\n",
    "    log_every_n_steps = 10\n",
    "\n",
    "    trainer = pl.Trainer(\n",
    "        accelerator=_config[\"accelerator\"],\n",
    "        devices=_config[\"devices\"],\n",
    "        num_nodes=_config[\"num_nodes\"],\n",
    "        precision=_config[\"precision\"],\n",
    "        strategy=strategy,\n",
    "        benchmark=True,\n",
    "        max_epochs=_config[\"max_epochs\"],\n",
    "        max_steps=max_steps,\n",
    "        callbacks=callbacks,\n",
    "        logger=logger,\n",
    "        accumulate_grad_batches=accumulate_grad_batches,\n",
    "        log_every_n_steps=log_every_n_steps,\n",
    "        val_check_interval=_config[\"val_check_interval\"],\n",
    "        deterministic=True,\n",
    "    )\n",
    "\n",
    "    if not _config[\"test_only\"]:\n",
    "        trainer.fit(model, datamodule=dm, ckpt_path=_config[\"resume_from\"])\n",
    "        log_dir = Path(logger.log_dir) / \"checkpoints\"\n",
    "        if best_model := next(log_dir.glob(\"epoch=*.ckpt\")):\n",
    "            shutil.copy(best_model, log_dir / \"best.ckpt\")\n",
    "\n",
    "    else:\n",
    "        trainer.test(model, datamodule=dm)\n",
    "\n",
    "\n",
    "import sys\n",
    "import os\n",
    "import copy\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "import re\n",
    "import csv\n",
    "\n",
    "import pytorch_lightning as pl\n",
    "\n",
    "from moftransformer.config import ex\n",
    "from moftransformer.config import config as _config\n",
    "from moftransformer.modules.module_utils import set_task\n",
    "from moftransformer.utils.validation import (\n",
    "    get_valid_config,\n",
    "    get_num_devices,\n",
    "    ConfigurationError,\n",
    "    _IS_INTERACTIVE,\n",
    ")\n",
    "\n",
    "warnings.filterwarnings(\n",
    "    \"ignore\", \".*Trying to infer the `batch_size` from an ambiguous collection.*\"\n",
    ")\n",
    "\n",
    "\n",
    "def predict(\n",
    "    root_dataset, load_path, downstream=None, split=\"all\", save_dir=None, **kwargs\n",
    "):\n",
    "\n",
    "    config = copy.deepcopy(_config())\n",
    "    for key in kwargs.keys():\n",
    "        if key not in config:\n",
    "            raise ConfigurationError(f\"{key} is not in configuration.\")\n",
    "\n",
    "    config.update(kwargs)\n",
    "    config[\"root_dataset\"] = root_dataset\n",
    "    config[\"downstream\"] = downstream\n",
    "    config[\"load_path\"] = load_path\n",
    "    config[\"test_only\"] = True\n",
    "    config[\"visualize\"] = False\n",
    "    config[\"split\"] = split\n",
    "    config[\"save_dir\"] = save_dir\n",
    "\n",
    "    return main_(config)\n",
    "\n",
    "\n",
    "# @ex.automain\n",
    "def main_(_config):\n",
    "    config = copy.deepcopy(_config)\n",
    "\n",
    "    config[\"test_only\"] = True\n",
    "    config[\"visualize\"] = False\n",
    "\n",
    "    os.makedirs(config[\"log_dir\"], exist_ok=True)\n",
    "    pl.seed_everything(config[\"seed\"])\n",
    "\n",
    "    num_device = get_num_devices(config)\n",
    "    num_nodes = config[\"num_nodes\"]\n",
    "    if num_nodes > 1:\n",
    "        warnings.warn(\n",
    "            f\"function <predict> only support 1 devices. change num_nodes {num_nodes} -> 1\"\n",
    "        )\n",
    "        config[\"num_nodes\"] = 1\n",
    "    if num_device > 1:\n",
    "        warnings.warn(\n",
    "            f\"function <predict> only support 1 devices. change num_devices {num_device} -> 1\"\n",
    "        )\n",
    "        config[\"devices\"] = 1\n",
    "\n",
    "    config = get_valid_config(config)  # valid config\n",
    "    model = Module1(config)\n",
    "    dm = Datamodule1(config)\n",
    "    model.eval()\n",
    "\n",
    "    if _IS_INTERACTIVE:\n",
    "        strategy = None\n",
    "    elif pl.__version__ >= \"2.0.0\":\n",
    "        strategy = \"ddp_find_unused_parameters_true\"\n",
    "    else:\n",
    "        strategy = \"ddp\"\n",
    "\n",
    "    trainer = pl.Trainer(\n",
    "        accelerator=config[\"accelerator\"],\n",
    "        devices=config[\"devices\"],\n",
    "        num_nodes=config[\"num_nodes\"],\n",
    "        precision=config[\"precision\"],\n",
    "        strategy=strategy,\n",
    "        benchmark=True,\n",
    "        max_epochs=1,\n",
    "        log_every_n_steps=0,\n",
    "        deterministic=True,\n",
    "        logger=False,\n",
    "    )\n",
    "\n",
    "    # refine split\n",
    "    split = config.get(\"split\", \"all\")\n",
    "    if split == \"all\":\n",
    "        split = [\"train\", \"val\", \"test\"]\n",
    "    elif isinstance(split, str):\n",
    "        split = re.split(r\",\\s?\", split)\n",
    "\n",
    "    if split == [\"test\"]:\n",
    "        dm.setup(\"test\")\n",
    "    elif \"test\" not in split:\n",
    "        dm.setup(\"fit\")\n",
    "    else:\n",
    "        dm.setup()\n",
    "\n",
    "    # save_dir\n",
    "    save_dir = config.get(\"save_dir\", None)\n",
    "    if save_dir is None:\n",
    "        save_dir = Path(config[\"load_path\"]).parent.parent\n",
    "    else:\n",
    "        save_dir = Path(save_dir)\n",
    "        save_dir.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "    # predict\n",
    "    for s in split:\n",
    "        if not s in [\"train\", \"test\", \"val\"]:\n",
    "            raise ValueError(f\"split must be train, test, or val, not {s}\")\n",
    "\n",
    "        savefile = save_dir / f\"{s}_prediction.csv\"\n",
    "        dataloader = getattr(dm, f\"{s}_dataloader\")()\n",
    "        rets = trainer.predict(model, dataloader)\n",
    "        write_output(rets, savefile)\n",
    "\n",
    "    print(f\"All prediction values are saved in {save_dir}\")\n",
    "    return rets\n",
    "\n",
    "\n",
    "def write_output(rets, savefile):\n",
    "    keys = rets[0].keys()\n",
    "\n",
    "    with open(savefile, \"w\") as f:\n",
    "        wr = csv.writer(f)\n",
    "        wr.writerow(keys)\n",
    "        for ret in rets:\n",
    "            if ret.keys() != keys:\n",
    "                raise ValueError(ret.keys(), keys)\n",
    "\n",
    "            for data in zip(*ret.values()):\n",
    "                wr.writerow(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78e042af-903e-4383-8716-8cbe90027d7a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Classification task, conductivity mechanism\n",
    "def main():\n",
    "    for seed in range(10):\n",
    "        for i in range(5):\n",
    "            load_path = \"./path/to/model/weights\"\n",
    "            save_dir = os.path.join(root_dataset, \"results/classification\")\n",
    "            preds = predict(\n",
    "                root_dataset=root_dataset,\n",
    "                load_path=load_path,\n",
    "                downstream=\"Ea_cls\",\n",
    "                loss_names=\"classification\",\n",
    "                n_classes=2,\n",
    "                split=[\"test\"],\n",
    "                save_dir=save_dir,\n",
    "            )\n",
    "            os.rename(\n",
    "                os.path.join(save_dir, \"test_prediction.csv\"),\n",
    "                os.path.join(save_dir, f\"test_prediction_fold_{i}_seed_{seed}.csv\"),\n",
    "            )\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pmf",
   "language": "python",
   "name": "pmf"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
